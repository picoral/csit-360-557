[
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "Topic: Introduction\n\n\n\nSet up your coding environment\nWhat Is Machine Learning?\n\n\n\n\n\nCourse Introduction\nAdvanced Methods Intro\nData\n\n\n\n\n\nBeginning of semester survey"
  },
  {
    "objectID": "modules.html#reading",
    "href": "modules.html#reading",
    "title": "Modules",
    "section": "",
    "text": "Set up your coding environment\nWhat Is Machine Learning?"
  },
  {
    "objectID": "modules.html#slides",
    "href": "modules.html#slides",
    "title": "Modules",
    "section": "",
    "text": "Course Introduction\nAdvanced Methods Intro\nData"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course website was created by Adriana Picoral and is licensed under the CC BY-NC-SA 4.0 Creative Commons License.\nYou are free to:\n\nShare — copy and redistribute the material in any medium or format.\nAdapt — remix, transform, and build upon the material.\n\nUnder the following terms:\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made.\nNon Commercial — You may not use the material for commercial purposes. (Remove this line if commercial use is permitted.)\nShare Alike — If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. (Include this if Share Alike applies.)"
  },
  {
    "objectID": "slides-01-01.html#hello",
    "href": "slides-01-01.html#hello",
    "title": "Introduction",
    "section": "Hello",
    "text": "Hello\n\nAdriana Picoral (PhD, you can call me Adriana)\npicorala@montclair.edu\nOffice: Schmitt 374\nOffice hours: Tuesdays and Thursdays, 1:45pm to 3:30pm"
  },
  {
    "objectID": "slides-01-01.html#active-learning",
    "href": "slides-01-01.html#active-learning",
    "title": "Introduction",
    "section": "Active Learning",
    "text": "Active Learning\n\nI expect you to actively participate in class (that does not mean you have to speak to the entire group)\nLots of hands-on activities and discussions\nWe will be using Gradescope (more on this later) to submit in-class work and for homework assignments"
  },
  {
    "objectID": "slides-01-01.html#ice-breaker",
    "href": "slides-01-01.html#ice-breaker",
    "title": "Introduction",
    "section": "Ice Breaker",
    "text": "Ice Breaker\n\nFind at least one person to talk to.\nIntroduce yourself (name, major/program, anything else you’d like to share)\nWhat is something you are good at and that took many hours of practice?"
  },
  {
    "objectID": "slides-01-01.html#course-overview",
    "href": "slides-01-01.html#course-overview",
    "title": "Introduction",
    "section": "Course Overview",
    "text": "Course Overview\n\nIntroduction to advanced techniques in Data Science\nFocus on both concepts and examples\nFocus on hands-on, bring a laptop to class if you have one\nFocus on real data in assignments"
  },
  {
    "objectID": "slides-01-01.html#course-overview-1",
    "href": "slides-01-01.html#course-overview-1",
    "title": "Introduction",
    "section": "Course Overview",
    "text": "Course Overview\nWhat is missing in this image?"
  },
  {
    "objectID": "slides-01-01.html#course-overview-2",
    "href": "slides-01-01.html#course-overview-2",
    "title": "Introduction",
    "section": "Course Overview",
    "text": "Course Overview\nTopics:\n\nReview the cycle of data science: data wrangling, and visualization\nMajor topics: how to make data-driven inferences and decisions by using fundamental techniques in machine learning and neural networks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSIT-360-557 Syllabus",
    "section": "",
    "text": "Course Information\n\nMonday and Wednesday, 12:00 – 1:25pm\nMode: in-person, at School of Business 011\n\n\n\nCourse Description\nThis course provides the advanced techniques in Data Science. After learning data manipulating, processing, cleaning, and visualization, the major goals of this course are to learn how to make data-driven inferences and decisions by using fundamental techniques in machine learning, in Python language.\n\n\nInstructor\n\nAdriana Picoral, PhD\nOffice: Schmitt 374\nemail: picorala@montclair.edu\nOffice Hours (open door/drop in):\n\nTuesdays and Thursdays, 1:45pm to 3:30pm\n\n\n\n\nCourse Outcomes\nUpon completion of the course, students should be able to:\n\nCO1: understand the concepts and various phases of Data Science\nCO2: understand how to use Python data science libraries to demonstrate the data science skills and visualize data\nCO3: understand the concepts of fundamental techniques in machine learning and neural networks\nCO4: implement techniques of machine learning algorithms on data analysis using advanced libraries such as sci-kit learn and TensorFlow in Python\nCO5: apply data science concepts and skills to solve problems with real-world data sets\n\n\n\nCourse Schedule\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStart Date\nTopic\n\n\n\n\n01\nJanuary 22\nIntroduction\n\n\n02\nJanuary 29\nPython Review\n\n\n03\nFeb 05\nNumpy & Pandas\n\n\n04\nFebruary 12\nData Wrangling\n\n\n05\nFebruary 19\nSupervised Learning – Linear Regression vs. Logistic Regression (statsmodels)\n\n\n06\nFebruary 26\nSupervised Learning – Regression and Classification with sklearn\n\n\n07\nMarch 05\nSupervised Learning – Suport Vector Machines, and multiclass classification\n\n\n08\nMarch 17\nUnsupervised Learning – clustering (Kmeans)\n\n\n09\nMarch 24\nIntroduction to Neural Networks\n\n\n10\nMarch 31\nUnsupervised Learning\n\n\n11\nApril 07\nUnsupervised Learning\n\n\n12\nApril 14\nOther Advanced Techniques\n\n\n13\nApril 21\nOther Advanced Techniques\n\n\n14\nApril 28\nFinal Project Presentations\n\n\n\n\n\n\n\n\nGrading Breakdown\n\n\n\nAssessment Element\nPercentage of Final Grade\n\n\n\n\nIn-class Activities\n10%\n\n\nProjects\n30%\n\n\nQuizzes (5)\n30%\n\n\nGroup Project\n20%\n\n\nProject Presentation\n10%\n\n\n\n\n\nLate Work Policy\nStudents are expected to complete work on schedule. The late policy for all assignments is as follows:\n\n20% points off if submitted within 24 hours after the due date\n30% off if submitted 24-48 hours after the due date\nNo credit if submitted two days or more days after the due date unless prior arrangements are made with the instructor with acceptable reasons.\n\nNote: This is a firm policy and it will be automatically applied\n\n\nAttendance Policy\nAttendance is mandatory. All students must sign an attendance sheet at the beginning of class. Excused absences include illness or a serious personal crisis (a letter from the Dean of students is required). If you are missing class and have a reasonable excuse, contact me before lecture if you want to make up missed assessments.\n\n\nGrading Scale\n\n\n\nLetter Grade\nGrade Percentage\n\n\n\n\nA\n94-100%\n\n\nA-\n90-93%\n\n\nB+\n87-89%\n\n\nB\n84-86%\n\n\nB-\n80-83%\n\n\nC+\n77-79%\n\n\nC\n74-76%\n\n\nC-\n70-73%\n\n\nD+ (undergraduate only)\n67-69%\n\n\nD (undergraduate only)\n64-66%\n\n\nD- (undergraduate only)\n60-63%\n\n\nF\n0-59%\n\n\n\n\n\nAcademic Honesty and Integrity\nAcademic Honesty is a core University value. Take time to understand the University’s policy. Your questions about academic honesty are always welcome.\nAll work you submit for grading in this course must be your own. Submitting work that includes (minor and/or major) components that are not your own work is considered plagiarism. I recommend that when talking with others about the assignment, do not write anything down.\nKeep in mind that all assignments and practice problems provided in this course are meant to help you practice the skills that you will need to do well in all assessments (including on paper quizzes), so it is generally in your best interest to avoid taking shortcuts even on practice problems (which are ungraded).\nSharing your code with others (in addition to copying code from others) is considered a break of the academic integrity code (unauthorized assistance) as well.\n\n\nLand Acknowledgement\nWe respectfully acknowledge that Montclair State University occupies land in Lenapehoking, the traditional and expropriated territory of the Lenape. As a state institution, we recognize and support the sovereignty of New Jersey’s three state-recognized tribes: the Ramapough Lenape, Nanticoke Lenni-Lenape, and Powhatan Renape nations.\nWe recognize the sovereign nations of the Lenape diaspora elsewhere in North America, as well as other Indigenous individuals and communities now residing in New Jersey. By offering this land acknowledgement, we commit to addressing the historical legacies of Indigenous dispossession and dismantling practices of erasure that persist today. We recognize the resilience and persistence of contemporary Indigenous communities and their role in educating all of us about justice, equity, and the stewardship of the land throughout the generations.\n\n\nSubject to Change\nChanges may be made to this syllabus as needed to support student learning. Any updates will be announced in class or through course materials with advance notice."
  },
  {
    "objectID": "environment.html",
    "href": "environment.html",
    "title": "Coding Environment",
    "section": "",
    "text": "We will be applying the concepts in this course to hands-on coding applications using real world data. For this, you need to have Python 3.8+ installed in your computer (or a computer that you have access to). I also recommend using Visual Studio Code for your development environment. That is what I will be using during class demonstrations and lecture materials.\nAfter installing and opening VS code, go to the File menu option and then select the Open Folder... option. Open an empty folder that you created, and that you know the location of in your computer. Create a new file that ends with the .py extension. If you do not have Python install in your machine, VS code will prompt you to install it.\nFor the next step you will need to have bash in your machine. If you have a windows machine, you will have to install bash – I recommend you install git bash.\nNext step is to install sckit-learn, one of the packages that we will be working with in this course. To do this, you can open a bash terminal in your VS code and type the following:\npip install -U scikit-learn\nWait and read the terminal standard output to check if the installation was completed successfully."
  },
  {
    "objectID": "slides-01-01.html#coding-tools",
    "href": "slides-01-01.html#coding-tools",
    "title": "Introduction",
    "section": "Coding tools",
    "text": "Coding tools\n\nWe will be coding in Python for this class.\nFor next lecture (Monday) you are expected to have set up your coding environment."
  },
  {
    "objectID": "slides-01-01.html#what-is-machine-learning",
    "href": "slides-01-01.html#what-is-machine-learning",
    "title": "Introduction",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nsubfield of computer science + statistics\nbuilding algorithms/models based on data (i.e. collection of examples of some phenomenon)"
  },
  {
    "objectID": "slides-01-01.html#types-of-machine-learning",
    "href": "slides-01-01.html#types-of-machine-learning",
    "title": "Introduction",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nsupervised (labeled data)\nsemi-supervised (both labeled and unlabeled data)\nunsupervised (unlabeled data)\nreinforcement (reward system, with the goal of learning a policy)\n\nFor all of these, data is required"
  },
  {
    "objectID": "slides-01-02.html#tidy-data",
    "href": "slides-01-02.html#tidy-data",
    "title": "Data",
    "section": "Tidy Data",
    "text": "Tidy Data\nThe data we will be working with contains variables as columns and observations as rows"
  },
  {
    "objectID": "slides-01-02.html#matrices-and-vectors",
    "href": "slides-01-02.html#matrices-and-vectors",
    "title": "Data",
    "section": "Matrices and Vectors",
    "text": "Matrices and Vectors\nFor machine learning, we have a feature matrix that contains all of our predictor variables, and a target vector that contains the label or target for each observation."
  },
  {
    "objectID": "slides-01-02.html#what-is-machine-learning",
    "href": "slides-01-02.html#what-is-machine-learning",
    "title": "Advanced Methods in Data Science",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nsubfield of computer science + statistics\nbuilding algorithms/models based on data (i.e. collection of examples of some phenomenon)"
  },
  {
    "objectID": "slides-01-02.html#types-of-machine-learning",
    "href": "slides-01-02.html#types-of-machine-learning",
    "title": "Advanced Methods in Data Science",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nsupervised (labeled data)\nsemi-supervised (both labeled and unlabeled data)\nunsupervised (unlabeled data)\nreinforcement (reward system, with the goal of learning a policy)\n\nFor all of these, data is required – one important step is feature engineering (how to transform data into feature – more on this later)\nDepending on the target type, classification or regression"
  },
  {
    "objectID": "slides-01-02.html#supervised-learning",
    "href": "slides-01-02.html#supervised-learning",
    "title": "Advanced Methods in Data Science",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nLabeled data – input (features), output (target, response)\n\nCan you think of examples of supervised learning?"
  },
  {
    "objectID": "slides-01-02.html#supervised-learning-1",
    "href": "slides-01-02.html#supervised-learning-1",
    "title": "Advanced Methods in Data Science",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nExample: spam vs. not spam\n\nWhat features can we use?"
  },
  {
    "objectID": "slides-01-02.html#supervised-learning-algorithms",
    "href": "slides-01-02.html#supervised-learning-algorithms",
    "title": "Advanced Methods in Data Science",
    "section": "Supervised Learning Algorithms",
    "text": "Supervised Learning Algorithms\n\nSupport Vector Machines (SVM)\nDecision Tree/Random Forest\nLogistic Regression\nLinear Regression\nNaive Bayes\nK-Nearest Neighbors\n\nHow to decide?"
  },
  {
    "objectID": "slides-01-02.html#unsupervised-learning",
    "href": "slides-01-02.html#unsupervised-learning",
    "title": "Advanced Methods in Data Science",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nClustering (K-Means)\nPrincipal Component Analysis (Dimensionality Reduction)"
  },
  {
    "objectID": "slides-01-02.html#reinforcement-learning",
    "href": "slides-01-02.html#reinforcement-learning",
    "title": "Advanced Methods in Data Science",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\nRecommendation Systems (users give feed on whether a recommendation was good or not)\nAutomated Robots\nAutonomous Driving\nNatural Language Processing – text prediction, translation"
  },
  {
    "objectID": "slides-01-03.html#data",
    "href": "slides-01-03.html#data",
    "title": "Data",
    "section": "Data",
    "text": "Data\nThe data we will be working with contains variables as columns and observations as rows (often called tidy data)\nFeature engineering: data –&gt; features\nRemember the spam vs not spam example? How do we go from words to features?"
  },
  {
    "objectID": "slides-01-03.html#data-quality",
    "href": "slides-01-03.html#data-quality",
    "title": "Data",
    "section": "Data quality",
    "text": "Data quality\nWhat to look for:\n\na data dictionary\ninformation on how the data were collected"
  },
  {
    "objectID": "slides-01-03.html#data-format",
    "href": "slides-01-03.html#data-format",
    "title": "Data",
    "section": "Data format",
    "text": "Data format\n\ntabular – tables, rows and columns\nhierachical – values are nested (like a tree)\nunstructured data – no structure, for example: emails, videos, pictures"
  },
  {
    "objectID": "slides-01-03.html#tabular-data",
    "href": "slides-01-03.html#tabular-data",
    "title": "Data",
    "section": "Tabular Data",
    "text": "Tabular Data\nRows and Columns\n\n\n\nDay\nHigh\nLow\nWind\nForecast\n\n\n\n\nTuesday\n24\n15\n0 to 15 mph\nSunny\n\n\nWednesday\n38\n17\n5 to 15 mph\nMostly Sunny\n\n\nThursday\n34\n13\n5 to 15 mph\nMostly Sunny"
  },
  {
    "objectID": "slides-01-03.html#hierachical-data",
    "href": "slides-01-03.html#hierachical-data",
    "title": "Data",
    "section": "Hierachical Data",
    "text": "Hierachical Data\nTuesday:\n   ↳ Temperature:\n      ↳ Low: 15\n      ↳ High: 24\n   ↳ Wind:\n      ↳ Speed: 0 to 15 mph \n      ↳ Direction: West\nWednesday:\n   ↳ Temperature:\n      ↳ Low: 17\n      ↳ High: 38\n   ↳ Wind:\n      ↳ Speed: 5 to 15 mph\n      ↳ Direction: North West"
  },
  {
    "objectID": "slides-01-03.html#unstructured-data",
    "href": "slides-01-03.html#unstructured-data",
    "title": "Data",
    "section": "Unstructured Data",
    "text": "Unstructured Data\nOne winter, I became very quiet\nand saw my life. It was February\n\nand outside in the city streets,\nsnow fell but would not collect.\n\nI bought snapdragons and thistle,\ngot some discount peach roses\n\nthat smelled off. I split them\nbetween vases and moved\n\nthe bouquets from room to room\nwhile a violin solo rang out.\nfull poem"
  },
  {
    "objectID": "slides-01-03.html#matrices-and-vectors",
    "href": "slides-01-03.html#matrices-and-vectors",
    "title": "Data",
    "section": "Matrices and Vectors",
    "text": "Matrices and Vectors\nFor machine learning, we have a feature matrix that contains all of our predictor variables, and a target vector that contains the label or target for each observation."
  },
  {
    "objectID": "slides-01-03.html#training-validation-and-testing",
    "href": "slides-01-03.html#training-validation-and-testing",
    "title": "Data",
    "section": "Training, Validation and Testing",
    "text": "Training, Validation and Testing\nWe usually need enough data to split it into training and validation (and testing)\n\nMost of our data will be used to build our model (training)\nWe never predict data that was in our training data set\nOur validation data set is used to fine tune our model (we can also do cross-validation)\nThe test data set is used to assess the final mode that has been selected during the validation process\n\nThe goal is to not overfit our data to our model"
  },
  {
    "objectID": "slides-01-02.html",
    "href": "slides-01-02.html",
    "title": "Advanced Methods in Data Science",
    "section": "",
    "text": "What is it? (discuss)\n\n\n\nsubfield of computer science + statistics\nbuilding algorithms/models based on data (i.e. collection of examples of some phenomenon)\n\n\n\n\n\nsupervised (labeled data) – classification and regression\nsemi-supervised (both labeled and unlabeled data)\nunsupervised (unlabeled data)\nreinforcement (reward system, with the goal of learning a policy)\n\nFor all of these, data is required – one important step is feature engineering (how to transform data into feature – more on this later)\n\n\n\n\nLabeled data – input (features), output (target, response)\n\nCan you think of examples of supervised learning?\n\n\n\n\nExample: spam vs. not spam\n\nWhat features can we use?\n\n\n\n\nSupport Vector Machines (SVM)\nDecision Tree/Random Forest\nLogistic Regression\nLinear Regression\nNaive Bayes\nK-Nearest Neighbors\n\nHow to decide?\n\n\n\n\nClustering (K-Means)\nPrincipal Component Analysis (Dimensionality Reduction)\n\n\n\n\n\nRecommendation Systems (users give feed on whether a recommendation was good or not)\nAutomated Robots\nAutonomous Driving\nNatural Language Processing – text prediction, translation"
  },
  {
    "objectID": "slides-01-01.html",
    "href": "slides-01-01.html",
    "title": "Introduction",
    "section": "",
    "text": "Adriana Picoral (PhD, you can call me Adriana)\npicorala@montclair.edu\nOffice:\nOffice hours: Tuesdays and Thursdays, 1:45pm to 3:30pm"
  },
  {
    "objectID": "environment.html#trouble-shooting",
    "href": "environment.html#trouble-shooting",
    "title": "Coding Environment",
    "section": "Trouble Shooting",
    "text": "Trouble Shooting\n\nRun where python and which python to find which Python installation your terminal has associated with different python aliases.\nYou can install a package to a specific python installation by running something like this: /usr/local/bin/python3 -m pip install statsmodels"
  },
  {
    "objectID": "environment.html#troubleshooting",
    "href": "environment.html#troubleshooting",
    "title": "Coding Environment",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nRun where python and which python to find which Python installation your terminal has associated with different python aliases.\nYou can install a package to a specific python installation by running something like this: /usr/local/bin/python3 -m pip install statsmodels"
  },
  {
    "objectID": "modules.html#assignments",
    "href": "modules.html#assignments",
    "title": "Modules",
    "section": "",
    "text": "Beginning of semester survey"
  },
  {
    "objectID": "slides-01-03.html#practice",
    "href": "slides-01-03.html#practice",
    "title": "Data",
    "section": "Practice",
    "text": "Practice\n\nAccess this kaggle dataset on house prices\nWhat are the variables in the data? (is there a data dictionary?)\nWhat is this data set from? What’s its source?\nAny problems you see with it?\nWhich variable could we use as target (response)?\nWhich variables would you use as features? Any feature engineering you can think of?"
  },
  {
    "objectID": "modules.html#video-lessons",
    "href": "modules.html#video-lessons",
    "title": "Modules",
    "section": "Video Lessons",
    "text": "Video Lessons\n\nOpening data files in VS Code Passcode: Uu5.@XE*"
  },
  {
    "objectID": "pandas-reference.html",
    "href": "pandas-reference.html",
    "title": "Pandas – reference sheet",
    "section": "",
    "text": "Import the package\n\nimport pandas\n\nYou can provide a shorter alias, which makes it easier to type\n\nimport pandas as pd\n\n\n\nLoad Data"
  },
  {
    "objectID": "slides-02-01.html",
    "href": "slides-02-01.html",
    "title": "Intro to Pandas",
    "section": "",
    "text": "Open a folder (that you have created, with the data we are going to be using) in VS code"
  },
  {
    "objectID": "slides-02-01.html#import-the-package",
    "href": "slides-02-01.html#import-the-package",
    "title": "Intro to Pandas",
    "section": "Import the package",
    "text": "Import the package\n\nimport pandas\n\nYou can provide a shorter alias, which makes it easier to type\n\nimport pandas as pd"
  },
  {
    "objectID": "slides-02-01.html#load-data",
    "href": "slides-02-01.html#load-data",
    "title": "Intro to Pandas",
    "section": "Load Data",
    "text": "Load Data\nLet’s use this kaggle dataset on house prices as an example. I downloaded the data and saved it in a folder called data.\n\ndata_frame = pd.read_csv(\"data/US houuse price of 10 states.csv\")"
  },
  {
    "objectID": "slides-02-01.html#inspect-the-data",
    "href": "slides-02-01.html#inspect-the-data",
    "title": "Intro to Pandas",
    "section": "Inspect the data",
    "text": "Inspect the data\n\ndata_frame.head()\ndata_frame.info()\ndata_frame.describe()\ndata_frame.shape\ndata_frame.columns\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12075 entries, 0 to 12074\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   date        12075 non-null  object\n 1   house_size  11107 non-null  object\n 2   bed         11410 non-null  object\n 3   bath        11410 non-null  object\n 4   price       11009 non-null  object\n 5   broker      9569 non-null   object\n 6   street      12075 non-null  object\n 7   city        12075 non-null  object\n 8   state_name  12075 non-null  object\n 9   zip_code    12075 non-null  int64 \ndtypes: int64(1), object(9)\nmemory usage: 943.5+ KB\n\n\nIndex(['date', 'house_size', 'bed', 'bath', 'price', 'broker', 'street',\n       'city', 'state_name', 'zip_code'],\n      dtype='object')"
  },
  {
    "objectID": "slides-02-01.html#setting-up-your-coding-environment",
    "href": "slides-02-01.html#setting-up-your-coding-environment",
    "title": "Intro to Pandas",
    "section": "Setting up your coding environment",
    "text": "Setting up your coding environment\nOpen a folder (that you have created, with the data we are going to be using) in VS code"
  },
  {
    "objectID": "slides-02-01.html#inspect-variables",
    "href": "slides-02-01.html#inspect-variables",
    "title": "Intro to Pandas",
    "section": "Inspect variables",
    "text": "Inspect variables\n\ndata_frame[\"bed\"].head()\ndata_frame[[\"bed\", \"bath\"]].head()\n\n\n\n\n\n\n\n\nbed\nbath\n\n\n\n\n0\n4bd\n4bd\n\n\n1\n2bd\n2bd\n\n\n2\n3bd\n3bd\n\n\n3\nStudio\nStudio\n\n\n4\n3bd\n3bd"
  },
  {
    "objectID": "slides-02-01.html#inspect-variables-1",
    "href": "slides-02-01.html#inspect-variables-1",
    "title": "Intro to Pandas",
    "section": "Inspect variables",
    "text": "Inspect variables\n\ndata_frame[\"bed\"].count() # number of non-null values\ndata_frame[\"bed\"].nunique() # count of unique values\ndata_frame[\"bed\"].value_counts() # count for each unique value\n\nbed\n3bd       5172\n4bd       2998\n2bd       1797\n5bd        762\n1bd        290\nStudio     153\n6bd        150\n7bd         31\n8bd         31\n9bd          9\n12bd         4\n10bd         3\n14bd         2\n21bd         2\n15bd         2\n11bd         2\n16bd         1\n13bd         1\nName: count, dtype: int64"
  },
  {
    "objectID": "slides-02-01.html#pandas.series.str",
    "href": "slides-02-01.html#pandas.series.str",
    "title": "Intro to Pandas",
    "section": "pandas.Series.str",
    "text": "pandas.Series.str\nA pandas.Series is one column in our data frame\nRead the documentation on pandas.Series.str – how can we create a numeric variable based on the \"bed\" column?"
  },
  {
    "objectID": "modules.html#video-lesson",
    "href": "modules.html#video-lesson",
    "title": "Modules",
    "section": "Video Lesson",
    "text": "Video Lesson\n\nOpening data files in VS Code Passcode: Uu5.@XE*"
  },
  {
    "objectID": "modules.html#slides-1",
    "href": "modules.html#slides-1",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nIntro to Pandas\nIntro to Seaborn\nRegular Expressions"
  },
  {
    "objectID": "slides-02-02.html#set-up-your-coding-environment",
    "href": "slides-02-02.html#set-up-your-coding-environment",
    "title": "Intro to Seaborn",
    "section": "Set up your coding environment",
    "text": "Set up your coding environment\n\nCreate a new .ipynb in your VS code folder.\nYou might need to run pip install notebook (python3 -m pip install notebook)\nYou also might need to run pip install jupyterlab"
  },
  {
    "objectID": "slides-02-02.html#import-the-libraries-we-will-be-using",
    "href": "slides-02-02.html#import-the-libraries-we-will-be-using",
    "title": "Intro to Seaborn",
    "section": "Import the libraries we will be using",
    "text": "Import the libraries we will be using\n\nimport seaborn as sns\n\nMake sure you import pandas and read the data in:\n\nimport pandas as pd\ndata_frame = pd.read_csv(\"data/US houuse price of 10 states.csv\")"
  },
  {
    "objectID": "slides-02-02.html#scatter-plot",
    "href": "slides-02-02.html#scatter-plot",
    "title": "Intro to Seaborn",
    "section": "Scatter plot",
    "text": "Scatter plot\n\nsns.scatterplot(data = data_frame, x = \"bed_numeric\", y = \"price_numeric\")"
  },
  {
    "objectID": "slides-02-02.html",
    "href": "slides-02-02.html",
    "title": "Intro to Seaborn",
    "section": "",
    "text": "Create a new .ipynb in your VS code folder.\nYou might need to run pip install notebook (python3 -m pip install notebook)\nYou also might need to run pip install jupyterlab"
  },
  {
    "objectID": "slides-02-01.html#filtering-the-data",
    "href": "slides-02-01.html#filtering-the-data",
    "title": "Intro to Pandas",
    "section": "Filtering the data",
    "text": "Filtering the data\n\ndata_frame[data_frame[\"bed\"] == \"3bd\"]\n\n\n\n\n\n\n\n\ndate\nhouse_size\nbed\nbath\nprice\nbroker\nstreet\ncity\nstate_name\nzip_code\n\n\n\n\n2\nAUG 29, 2024\n1,926 sqft (on 0.45 acres)\n3bd\n3bd\n$375,000\nColdwell Banker Hartung\n6761 Landover Cir\nTallahassee\nFlorida\n32317\n\n\n4\nAUG 29, 2024\n1,205 sqft\n3bd\n3bd\n$233,900\nD R Horton Realty of NW Florida, LLC\n6274 June Bug Dr\nMilton\nFlorida\n32583\n\n\n14\nAUG 28, 2024\n1,820 sqft\n3bd\n3bd\n$330,500\nEXP Realty, LLC\n8564 Westview Ln\nPensacola\nFlorida\n32514\n\n\n15\nAUG 28, 2024\n1,370 sqft\n3bd\n3bd\n$173,000\nBetter Homes And Gardens Real Estate Main Stre...\n6905 Woodley Dr\nPensacola\nFlorida\n32503\n\n\n17\nAUG 28, 2024\n2,681 sqft (on 1.82 acres)\n3bd\n3bd\n$525,000\nAmerican Valor Realty LLC\n8021 Quiet Dr\nPensacola\nFlorida\n32526\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12059\nAUG 30, 2024\n2,610 sqft\n3bd\n3bd\n$1,143,909\nBALBOA REAL ESTATE, INC.\n50525 Spyglass Hill Dr\nLa Quinta\nCA\n92253\n\n\n12063\nAUG 30, 2024\n1,983 sqft (on 1 acre)\n3bd\n3bd\n$1,385,000\nCompass\n10241 McBroom St\nSunland\nCA\n91040\n\n\n12065\nAUG 30, 2024\n1,757 sqft\n3bd\n3bd\n$568,000\nBerkshire Hathaway Home Serv.\n26848 Hanford St\nMenifee\nCA\n92584\n\n\n12069\nAUG 30, 2024\n2,000 sqft\n3bd\n3bd\n$1,880,000\nReal Estate Legends USA\n16 Riveroaks\nIrvine\nCA\n92602\n\n\n12074\nAUG 30, 2024\n1,615 sqft\n3bd\n3bd\n$508,000\nStarlitloan&Realty\n1668 Ravenswood Rd\nBeaumont\nCA\n92223\n\n\n\n\n5172 rows × 10 columns"
  },
  {
    "objectID": "slides-02-01.html#handling-missing-data-drop-missing-data",
    "href": "slides-02-01.html#handling-missing-data-drop-missing-data",
    "title": "Intro to Pandas",
    "section": "Handling Missing Data – drop missing data",
    "text": "Handling Missing Data – drop missing data\nDocumentation on .dropna()\n\ndata_frame.dropna()\n\n\n\n\n\n\n\n\ndate\nhouse_size\nbed\nbath\nprice\nbroker\nstreet\ncity\nstate_name\nzip_code\n\n\n\n\n2\nAUG 29, 2024\n1,926 sqft (on 0.45 acres)\n3bd\n3bd\n$375,000\nColdwell Banker Hartung\n6761 Landover Cir\nTallahassee\nFlorida\n32317\n\n\n3\nAUG 29, 2024\n1,132 sqft\nStudio\nStudio\n$190,000\nEXP Realty, LLC\n1701 S Fairfield Dr\nPerdido Key\nFlorida\n32507\n\n\n4\nAUG 29, 2024\n1,205 sqft\n3bd\n3bd\n$233,900\nD R Horton Realty of NW Florida, LLC\n6274 June Bug Dr\nMilton\nFlorida\n32583\n\n\n5\nAUG 29, 2024\n3,044 sqft (on 0.34 acres)\n4bd\n4bd\n$416,402\nADAMS HOME REALTY, INC\n6528 Benelli Dr\nMilton\nFlorida\n32570\n\n\n13\nAUG 28, 2024\n1,254 sqft\n2bd\n2bd\n$250,000\nJANET COULTER REALTY\n520 Richard Jackson Blvd #2810\nPanama City Beach\nFlorida\n32407\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12067\nAUG 30, 2024\n2,352 sqft\n4bd\n4bd\n$795,000\nRealty Masters\n12549 Navel Ct\nRiverside\nCA\n92503\n\n\n12069\nAUG 30, 2024\n2,000 sqft\n3bd\n3bd\n$1,880,000\nReal Estate Legends USA\n16 Riveroaks\nIrvine\nCA\n92602\n\n\n12070\nAUG 30, 2024\n3,835 sqft\n5bd\n5bd\n$1,935,000\nRealty ONE Group Pacific\n1154 Via Vera Cruz\nSan Marcos\nCA\n92078\n\n\n12072\nAUG 30, 2024\n2,616 sqft\n4bd\n4bd\n$685,000\nAnderson Real Estate Group\n5509 W Modoc Avenue\nVisalia\nCA\n93291\n\n\n12074\nAUG 30, 2024\n1,615 sqft\n3bd\n3bd\n$508,000\nStarlitloan&Realty\n1668 Ravenswood Rd\nBeaumont\nCA\n92223\n\n\n\n\n8068 rows × 10 columns"
  },
  {
    "objectID": "slides-02-01.html#handling-missing-data-fill-missing-data",
    "href": "slides-02-01.html#handling-missing-data-fill-missing-data",
    "title": "Intro to Pandas",
    "section": "Handling Missing Data – fill missing data",
    "text": "Handling Missing Data – fill missing data\nDocumentation on .fillna()\n\ndata_frame.fillna(0)\ndata_frame.ffill()\ndata_frame.bfill()\ndata_frame.fillna(data_frame.mean())\ndata_frame[\"bed_numeric\"].fillna(data_frame[\"bed_numeric\"].mean())"
  },
  {
    "objectID": "slides-02-03.html#regex",
    "href": "slides-02-03.html#regex",
    "title": "Regular Expressions",
    "section": "Regex",
    "text": "Regex\nRegular expressions are:\n\npatterns used to match and manipulate text strings"
  },
  {
    "objectID": "slides-02-03.html#other-syntax",
    "href": "slides-02-03.html#other-syntax",
    "title": "Regular Expressions",
    "section": "Other syntax",
    "text": "Other syntax\n\n\\d - matches any digit (0-9)\n\\w - matches any word character (a-z, A-Z, 0-9, _)\n\\s - matches any whitespace character (space, tab, newline)\n[] used to indicate a set of characters.\n{m} matches m times\n{m,n} matches m to n times\n\ne.g., lower case letters: [a-z], upper case letters: [A-Z]"
  },
  {
    "objectID": "slides-02-03.html#python-examples",
    "href": "slides-02-03.html#python-examples",
    "title": "Regular Expressions",
    "section": "Python examples",
    "text": "Python examples\nEmail validation pattern:\n\npattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n\nPhone number pattern (US format):\n\npattern = r'\\d{3}-\\d{3}-\\d{4}'"
  },
  {
    "objectID": "slides-02-03.html#basic-patterns",
    "href": "slides-02-03.html#basic-patterns",
    "title": "Regular Expressions",
    "section": "Basic Patterns",
    "text": "Basic Patterns\n\nLiteral characters match themselves (“cat” matches “cat”)\nSpecial characters have specific meanings:\n\n. (dot) - matches any single character except newline\n* matches 0 or more of the previous character\n+ matches 1 or more of the previous character\n? - matches 0 or 1 of the previous character\n^ - matches start of line\n$ - matches end of line"
  },
  {
    "objectID": "slides-02-03.html#examples",
    "href": "slides-02-03.html#examples",
    "title": "Regular Expressions",
    "section": "Examples",
    "text": "Examples\n[abc] - matches any single character from the set (a, b, or c)\n[^abc] - matches any single character NOT in the set"
  },
  {
    "objectID": "modules.html#practice",
    "href": "modules.html#practice",
    "title": "Modules",
    "section": "Practice",
    "text": "Practice\nClean this data set on Titanic survivors"
  },
  {
    "objectID": "slides-02-04.html",
    "href": "slides-02-04.html",
    "title": "Numpy",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "slides-02-04.html#package-and-data",
    "href": "slides-02-04.html#package-and-data",
    "title": "Numpy",
    "section": "Package and Data",
    "text": "Package and Data\n\nimport numpy as np"
  },
  {
    "objectID": "slides-02-04.html#recode-data",
    "href": "slides-02-04.html#recode-data",
    "title": "Numpy",
    "section": "Recode Data",
    "text": "Recode Data\n\nbank['sex'] = np.where(bank['sex'] == \"Female\", 0, 1)\nbank.head()"
  },
  {
    "objectID": "slides-02-04.html#transform-data",
    "href": "slides-02-04.html#transform-data",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\n\nbank['log_salary'] = np.log(bank['salary'])"
  },
  {
    "objectID": "slides-03-01.html#methods-to-use",
    "href": "slides-03-01.html#methods-to-use",
    "title": "Data Wrangling",
    "section": "Methods to use",
    "text": "Methods to use\nTwo .replace() methods, one from pandas.Series.str and another from pandas.DataFrame\n\ndata_frame[column_name] = data_frame[column_name].str.replace(r\"none\", \"0\")\ndata_frame[column_name] = pd.to_numeric(data_frame[column_name])\ndata_frame = data_frame.replace({'data_frame': {'value1': '0', 'value2': '1'}})"
  },
  {
    "objectID": "slides-03-01.html#practice",
    "href": "slides-03-01.html#practice",
    "title": "Data Wrangling",
    "section": "Practice",
    "text": "Practice\n\nClean this data set on Titanic survivors\nMake sure you define a main() methods that reads and writes the data\nJoin Gradescope – it’s free, and join our course (Entry Code:8XG2NV)\nSubmit your file(s) to the Data Wrangling assignment"
  },
  {
    "objectID": "slides-03-01.html#exploratory-data-analysis",
    "href": "slides-03-01.html#exploratory-data-analysis",
    "title": "Data Wrangling",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nCalculating descriptive stats of a numeric variable by a group.\n\ndata_frame.groupby(\"category\")[\"numeric_value\"].mean().reset_index()\ndata_frame.groupby(\"category\")[\"numeric_value\"].agg([\"mean\", \"std\", \"max\", \"min\"]).reset_index()"
  },
  {
    "objectID": "slides-03-01.html#bar-plots",
    "href": "slides-03-01.html#bar-plots",
    "title": "Data Wrangling",
    "section": "Bar plots",
    "text": "Bar plots\n\nimport seaborn as sns\n\nsns.barplot(data = desc_stats, x = \"category\", y = \"mean\")"
  },
  {
    "objectID": "modules.html#slides-2",
    "href": "modules.html#slides-2",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nData Wrangling\nNumpy\nData Modeling – intro"
  },
  {
    "objectID": "slides-03-02.html#data-modeling",
    "href": "slides-03-02.html#data-modeling",
    "title": "Intro to Data Modeling",
    "section": "Data Modeling",
    "text": "Data Modeling"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction",
    "href": "slides-03-02.html#inference-vs.-prediction",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference:\n\nunderstanding relationships and testing hypotheses about how variables interact\nexplain the underlying mechanisms and relationships in the data\nHow does education relates to income?"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-1",
    "href": "slides-03-02.html#inference-vs.-prediction-1",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nPrediction:\n\ngenerating accurate estimates of future or unknown values\noptimizing the model’s ability to make correct predictions\nAccurately forecast someone’s income based on their education level."
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-2",
    "href": "slides-03-02.html#inference-vs.-prediction-2",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference models prioritize interpretability and often use simpler, more transparent methods like linear regression\nPrediction models may use more complex “black box” approaches like neural networks if they improve accuracy"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-3",
    "href": "slides-03-02.html#inference-vs.-prediction-3",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nIn inference, you carefully choose variables based on theory and prior research\nFor prediction, you might include any feature that improves predictive performance, even if the relationship isn’t theoretically clear (! careful with this approach)"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-4",
    "href": "slides-03-02.html#inference-vs.-prediction-4",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference focuses on metrics like p-values, confidence intervals, and effect sizes\nPrediction emphasizes metrics like mean squared error, classification accuracy, or the area under the ROC curve (AUC)"
  },
  {
    "objectID": "slides-03-02.html#inference-vs.-prediction-5",
    "href": "slides-03-02.html#inference-vs.-prediction-5",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\nOverfitting concerns\n\nboth approaches need to address overfitting\nit’s especially critical in prediction\ninference models might accept slightly worse fit for better interpretability\nprediction models focus heavily on cross-validation and out-of-sample performance"
  },
  {
    "objectID": "slides-03-02.html#inference",
    "href": "slides-03-02.html#inference",
    "title": "Intro to Data Modeling",
    "section": "Inference",
    "text": "Inference\nFor interpreting statistical models, we will be using the statsmodels Python package"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nOLS is a method that finds the best-fitting straight line through a set of points by minimizing the sum of squared vertical distances between the data points and the line.\nThe goal is to find a line y = β₀ + β₁x that best fits your data, where:\n\nβ₀ is the y-intercept\nβ₁ is the slope\nx is your independent variable – or target\ny is your dependent variable – or feature(s)"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression-1",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression-1",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nFor each data point, OLS:\n\nCalculates the vertical distance (residual) between the actual y-value and the predicted y-value\nSquares these distances (to make negatives positive and penalize larger errors more)\nSums all squared distances\nFinds the line that minimizes this sum"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression-2",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression-2",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nKey Assumptions:\n\nLinear relationship between variables\nIndependent observations\nHomoscedasticity (constant variance in errors)\nNormally distributed errors\nNo perfect multicollinearity"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression-3",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression-3",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nAdvantages:\n\nSimple to understand and implement\nBest Linear Unbiased Estimator (BLUE) under certain conditions\nComputationally efficient\nClear interpretation of results"
  },
  {
    "objectID": "slides-03-02.html#ordinary-least-squares-ols-regression-4",
    "href": "slides-03-02.html#ordinary-least-squares-ols-regression-4",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nLimitations:\n\nSensitive to outliers\nAssumes linear relationships\nMay not capture complex patterns\nAll predictors must be independent"
  },
  {
    "objectID": "slides-03-02.html#ols-regression-in-statsmodels",
    "href": "slides-03-02.html#ols-regression-in-statsmodels",
    "title": "Intro to Data Modeling",
    "section": "OLS regression in statsmodels",
    "text": "OLS regression in statsmodels\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n\ndef main():\n    data = pd.read_csv(\"data/clean_titanic.csv\")\n    X = data[[\"pclass\", \"sex\"]]  \n    y = data[\"survived\"] \n\n    # Ordinary Least Squares (regression)\n    result = sm.OLS(y, sm.add_constant(X)).fit()\n    print(result.summary())\n\nmain()"
  },
  {
    "objectID": "slides-03-02.html#ols-regression-results",
    "href": "slides-03-02.html#ols-regression-results",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               survived   R-squared:                       0.366\nModel:                            OLS   Adj. R-squared:                  0.365\nMethod:                 Least Squares   F-statistic:                     255.2\nDate:                Tue, 11 Feb 2025   Prob (F-statistic):           3.19e-88\nTime:                        14:23:34   Log-Likelihood:                -417.77\nNo. Observations:                 887   AIC:                             841.5\nDf Residuals:                     884   BIC:                             855.9\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=============================================================================="
  },
  {
    "objectID": "slides-03-02.html#ols-regression-results-1",
    "href": "slides-03-02.html#ols-regression-results-1",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0825      0.040     26.795      0.000       1.003       1.162\npclass        -0.1577      0.016    -10.029      0.000      -0.189      -0.127\nsex           -0.5161      0.027    -18.776      0.000      -0.570      -0.462\n==============================================================================\nOmnibus:                       40.150   Durbin-Watson:                   1.921\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               44.755\nSkew:                           0.549   Prob(JB):                     1.91e-10\nKurtosis:                       3.060   Cond. No.                         9.07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "slides-03-02.html#ols-regression-results-2",
    "href": "slides-03-02.html#ols-regression-results-2",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n\nWhat other variables can you add to the model?\nHow does the model change?"
  },
  {
    "objectID": "slides-03-02.html#ols-regression-results-3",
    "href": "slides-03-02.html#ols-regression-results-3",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\nRun OLS regression on this kaggle dataset on house prices"
  },
  {
    "objectID": "slides-02-04.html#transform-data-1",
    "href": "slides-02-04.html#transform-data-1",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nNormalizing skewed distributions: Many real-world variables (like income, population, prices) tend to have right-skewed distributions, where most values are small but there are some very large values. Taking the logarithm can make these distributions more symmetric and closer to normal, which is beneficial for many statistical methods."
  },
  {
    "objectID": "slides-02-04.html#transform-data-2",
    "href": "slides-02-04.html#transform-data-2",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nManaging outliers: Log transformation reduces the impact of extreme values or outliers. For example, the difference between $1M and $2M becomes similar in magnitude to the difference between $10K and $20K when logged."
  },
  {
    "objectID": "slides-02-04.html#transform-data-3",
    "href": "slides-02-04.html#transform-data-3",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nLinearizing relationships: Many relationships that appear exponential become linear after log transformation. For instance, if \\(y = ax^b\\), then \\(log(y) = log(a) + b*log(x)\\) is linear. This makes the relationship easier to model and interpret."
  },
  {
    "objectID": "slides-02-04.html#transform-data-4",
    "href": "slides-02-04.html#transform-data-4",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nMaking multiplicative relationships additive: When variables have multiplicative effects (like compound growth), logging transforms them into additive relationships. This often makes more sense for how variables actually interact in real-world systems."
  },
  {
    "objectID": "slides-02-04.html#transform-data-5",
    "href": "slides-02-04.html#transform-data-5",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nStabilizing variance: When the variability of data increases with its magnitude (heteroscedasticity), log transformation can help create more constant variance across the range of values, which is important for many statistical methods."
  },
  {
    "objectID": "slides-03-02.html#package-and-data",
    "href": "slides-03-02.html#package-and-data",
    "title": "Numpy",
    "section": "Package and Data",
    "text": "Package and Data\n\nimport numpy as np"
  },
  {
    "objectID": "slides-03-02.html#recode-data",
    "href": "slides-03-02.html#recode-data",
    "title": "Numpy",
    "section": "Recode Data",
    "text": "Recode Data\n\nbank['sex'] = np.where(bank['sex'] == \"Female\", 0, 1)\nbank.head()"
  },
  {
    "objectID": "slides-03-02.html#transform-data",
    "href": "slides-03-02.html#transform-data",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\n\nbank['log_salary'] = np.log(bank['salary'])"
  },
  {
    "objectID": "slides-03-02.html#transform-data-1",
    "href": "slides-03-02.html#transform-data-1",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nNormalizing skewed distributions: Many real-world variables (like income, population, prices) tend to have right-skewed distributions, where most values are small but there are some very large values. Taking the logarithm can make these distributions more symmetric and closer to normal, which is beneficial for many statistical methods."
  },
  {
    "objectID": "slides-03-02.html#transform-data-2",
    "href": "slides-03-02.html#transform-data-2",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nManaging outliers: Log transformation reduces the impact of extreme values or outliers. For example, the difference between $1M and $2M becomes similar in magnitude to the difference between $10K and $20K when logged."
  },
  {
    "objectID": "slides-03-02.html#transform-data-3",
    "href": "slides-03-02.html#transform-data-3",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nLinearizing relationships: Many relationships that appear exponential become linear after log transformation. For instance, if \\(y = ax^b\\), then \\(log(y) = log(a) + b*log(x)\\) is linear. This makes the relationship easier to model and interpret."
  },
  {
    "objectID": "slides-03-02.html#transform-data-4",
    "href": "slides-03-02.html#transform-data-4",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nMaking multiplicative relationships additive: When variables have multiplicative effects (like compound growth), logging transforms them into additive relationships. This often makes more sense for how variables actually interact in real-world systems."
  },
  {
    "objectID": "slides-03-02.html#transform-data-5",
    "href": "slides-03-02.html#transform-data-5",
    "title": "Numpy",
    "section": "Transform Data",
    "text": "Transform Data\nWhen do we log-transform numeric variables?\n\nStabilizing variance: When the variability of data increases with its magnitude (heteroscedasticity), log transformation can help create more constant variance across the range of values, which is important for many statistical methods."
  },
  {
    "objectID": "slides-03-03.html#data-modeling",
    "href": "slides-03-03.html#data-modeling",
    "title": "Intro to Data Modeling",
    "section": "Data Modeling",
    "text": "Data Modeling"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction",
    "href": "slides-03-03.html#inference-vs.-prediction",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference:\n\nunderstanding relationships and testing hypotheses about how variables interact\nexplain the underlying mechanisms and relationships in the data\nHow does education relates to income?"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-1",
    "href": "slides-03-03.html#inference-vs.-prediction-1",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nPrediction:\n\ngenerating accurate estimates of future or unknown values\noptimizing the model’s ability to make correct predictions\nAccurately forecast someone’s income based on their education level."
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-2",
    "href": "slides-03-03.html#inference-vs.-prediction-2",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference models prioritize interpretability and often use simpler, more transparent methods like linear regression\nPrediction models may use more complex “black box” approaches like neural networks if they improve accuracy"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-3",
    "href": "slides-03-03.html#inference-vs.-prediction-3",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nIn inference, you carefully choose variables based on theory and prior research\nFor prediction, you might include any feature that improves predictive performance, even if the relationship isn’t theoretically clear (! careful with this approach)"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-4",
    "href": "slides-03-03.html#inference-vs.-prediction-4",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\nInference focuses on metrics like p-values, confidence intervals, and effect sizes\nPrediction emphasizes metrics like mean squared error, classification accuracy, or the area under the ROC curve (AUC)"
  },
  {
    "objectID": "slides-03-03.html#inference-vs.-prediction-5",
    "href": "slides-03-03.html#inference-vs.-prediction-5",
    "title": "Intro to Data Modeling",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\nOverfitting concerns\n\nboth approaches need to address overfitting\nit’s especially critical in prediction\ninference models might accept slightly worse fit for better interpretability\nprediction models focus heavily on cross-validation and out-of-sample performance"
  },
  {
    "objectID": "slides-03-03.html#inference",
    "href": "slides-03-03.html#inference",
    "title": "Intro to Data Modeling",
    "section": "Inference",
    "text": "Inference\nFor interpreting statistical models, we will be using the statsmodels Python package"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nOLS is a method that finds the best-fitting straight line through a set of points by minimizing the sum of squared vertical distances between the data points and the line.\nThe goal is to find a line y = β₀ + β₁x that best fits your data, where:\n\nβ₀ is the y-intercept\nβ₁ is the slope\nx is your independent variable – or feature(s)\ny is your dependent variable – or target"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression-1",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression-1",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nFor each data point, OLS:\n\nCalculates the vertical distance (residual) between the actual y-value and the predicted y-value\nSquares these distances (to make negatives positive and penalize larger errors more)\nSums all squared distances\nFinds the line that minimizes this sum"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression-2",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression-2",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nKey Assumptions:\n\nLinear relationship between variables\nIndependent observations\nHomoscedasticity (constant variance in errors)\nNormally distributed errors\nNo perfect multicollinearity"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression-3",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression-3",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nAdvantages:\n\nSimple to understand and implement\nBest Linear Unbiased Estimator (BLUE) under certain conditions\nComputationally efficient\nClear interpretation of results"
  },
  {
    "objectID": "slides-03-03.html#ordinary-least-squares-ols-regression-4",
    "href": "slides-03-03.html#ordinary-least-squares-ols-regression-4",
    "title": "Intro to Data Modeling",
    "section": "Ordinary Least Squares (OLS) regression",
    "text": "Ordinary Least Squares (OLS) regression\nLimitations:\n\nSensitive to outliers\nAssumes linear relationships\nMay not capture complex patterns\nAll predictors must be independent"
  },
  {
    "objectID": "slides-03-03.html#ols-regression-in-statsmodels",
    "href": "slides-03-03.html#ols-regression-in-statsmodels",
    "title": "Intro to Data Modeling",
    "section": "OLS regression in statsmodels",
    "text": "OLS regression in statsmodels\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n\ndef main():\n    data = pd.read_csv(\"data/clean_titanic.csv\")\n    X = data[[\"pclass\", \"sex\"]]  \n    y = data[\"survived\"] \n\n    # Ordinary Least Squares (regression)\n    result = sm.OLS(y, sm.add_constant(X)).fit()\n    print(result.summary())\n\nmain()"
  },
  {
    "objectID": "slides-03-03.html#ols-regression-results",
    "href": "slides-03-03.html#ols-regression-results",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               survived   R-squared:                       0.366\nModel:                            OLS   Adj. R-squared:                  0.365\nMethod:                 Least Squares   F-statistic:                     255.2\nDate:                Tue, 11 Feb 2025   Prob (F-statistic):           3.19e-88\nTime:                        14:23:34   Log-Likelihood:                -417.77\nNo. Observations:                 887   AIC:                             841.5\nDf Residuals:                     884   BIC:                             855.9\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=============================================================================="
  },
  {
    "objectID": "slides-03-03.html#ols-regression-results-1",
    "href": "slides-03-03.html#ols-regression-results-1",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0825      0.040     26.795      0.000       1.003       1.162\npclass        -0.1577      0.016    -10.029      0.000      -0.189      -0.127\nsex           -0.5161      0.027    -18.776      0.000      -0.570      -0.462\n==============================================================================\nOmnibus:                       40.150   Durbin-Watson:                   1.921\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               44.755\nSkew:                           0.549   Prob(JB):                     1.91e-10\nKurtosis:                       3.060   Cond. No.                         9.07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "slides-03-03.html#ols-regression-results-2",
    "href": "slides-03-03.html#ols-regression-results-2",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\n\nWhat other variables can you add to the model?\nHow does the model change?"
  },
  {
    "objectID": "slides-03-03.html#ols-regression-results-3",
    "href": "slides-03-03.html#ols-regression-results-3",
    "title": "Intro to Data Modeling",
    "section": "OLS regression results",
    "text": "OLS regression results\nRun OLS regression on this kaggle dataset on house prices"
  },
  {
    "objectID": "modules.html#slides-3",
    "href": "modules.html#slides-3",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nOLS case study\nLogistic Regression\nOLS vs. Logit – case study\nMachine Learning"
  },
  {
    "objectID": "slides-04-02.html#logistic-regression",
    "href": "slides-04-02.html#logistic-regression",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nBinary Outcome – statistical model used for binary classification problems\nPredicts probability of an event occurring (between 0 and 1)\nUses the logistic/sigmoid function to transform predictions"
  },
  {
    "objectID": "slides-04-02.html#the-logistic-function",
    "href": "slides-04-02.html#the-logistic-function",
    "title": "Logistic Regression",
    "section": "The Logistic Function",
    "text": "The Logistic Function\n\nS-shaped curve (sigmoid)\nTransforms any input into a probability between 0 and 1\nFormula: \\(p = \\frac{1}{1 + e^{-z}}\\) where z is the linear predictor \\(\\beta_0 + \\beta_1 * x\\)"
  },
  {
    "objectID": "slides-04-02.html#assumptions",
    "href": "slides-04-02.html#assumptions",
    "title": "Logistic Regression",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependent observations\nNo multicollinearity among predictors\nLinear relationship between target (in log odds) and predictors\nAdequate sample size"
  },
  {
    "objectID": "slides-04-02.html#data",
    "href": "slides-04-02.html#data",
    "title": "Logistic Regression",
    "section": "Data",
    "text": "Data\nDownload Tagliamonte’s that expresion data and inspect it\nPhenomenom:\n\nI think that I will have a great time\nI think I will have a great time\n\nQuestion: What linguistics and social factor affect the expression of that"
  },
  {
    "objectID": "slides-04-02.html#logistic-regression-with-statsmodels",
    "href": "slides-04-02.html#logistic-regression-with-statsmodels",
    "title": "Logistic Regression",
    "section": "Logistic Regression with statsmodels",
    "text": "Logistic Regression with statsmodels\nDownload clean-that-expression.csv\n\nimport statsmodels.api as sm \nimport pandas as pd  \n\ndef main():\n    # loading the training dataset  \n    data = pd.read_csv(\"data/clean-that-expression.csv\") \n    \n    # defining the dependent and independent variables \n    X = data[[\"know\"]] \n    y = data[\"expressed\"]\n    \n    # building the model and fitting the data \n    log_reg = sm.Logit(y, sm.add_constant(X)).fit() \n    print(log_reg.summary()) \n\nmain()"
  },
  {
    "objectID": "slides-04-01.html#house-prices",
    "href": "slides-04-01.html#house-prices",
    "title": "OLS Case Study",
    "section": "House Prices",
    "text": "House Prices\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef main():\n    data = pd.read_csv(\"data/clean_house_prices.csv\")\n\n    X = data[[\"bed\", \"house_size\"]]\n    y = data[\"price\"]\n\n    result = sm.OLS(y, sm.add_constant(X)).fit()\n    print(result.summary())\n\n\nmain()"
  },
  {
    "objectID": "slides-04-01.html#results-part-1",
    "href": "slides-04-01.html#results-part-1",
    "title": "OLS Case Study",
    "section": "Results – part 1",
    "text": "Results – part 1\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.242\nModel:                            OLS   Adj. R-squared:                  0.242\nMethod:                 Least Squares   F-statistic:                     1623.\nDate:                Sun, 16 Feb 2025   Prob (F-statistic):               0.00\nTime:                        09:06:45   Log-Likelihood:            -1.4998e+05\nNo. Observations:               10154   AIC:                         3.000e+05\nDf Residuals:                   10151   BIC:                         3.000e+05\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=============================================================================="
  },
  {
    "objectID": "slides-04-01.html#results-part-2",
    "href": "slides-04-01.html#results-part-2",
    "title": "OLS Case Study",
    "section": "Results – part 2",
    "text": "Results – part 2\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       -2.26e+04   1.92e+04     -1.177      0.239   -6.02e+04     1.5e+04\nbed        -2.675e+04   7072.066     -3.783      0.000   -4.06e+04   -1.29e+04\nhouse_size   391.1845      8.311     47.067      0.000     374.893     407.476\n==============================================================================\nOmnibus:                    12672.728   Durbin-Watson:                   1.202\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3902146.529\nSkew:                           6.543   Prob(JB):                         0.00\nKurtosis:                      98.141   Cond. No.                     6.85e+03\n=============================================================================="
  },
  {
    "objectID": "slides-04-01.html#interpretation",
    "href": "slides-04-01.html#interpretation",
    "title": "OLS Case Study",
    "section": "Interpretation",
    "text": "Interpretation\nFor every added square foot, the price of the house goes up by 391.1845 dollars"
  },
  {
    "objectID": "slides-04-01.html#predicting-new-data",
    "href": "slides-04-01.html#predicting-new-data",
    "title": "OLS Case Study",
    "section": "Predicting new data",
    "text": "Predicting new data\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef main():\n    data = pd.read_csv(\"data/clean_house_prices.csv\")\n\n    X = data[[\"bed\", \"house_size\", \"zip_code_99350\"]]\n    y = data[\"price\"]\n\n    result = sm.OLS(y, sm.add_constant(X)).fit()\n    print(result.summary())\n\n    data_dict = {\"bed\": [2, 3, 2], \n                 \"house_size\": [1400, 2300, 1400],\n                 \"zip_code_99350\": [0, 0, 1] }\n    df_dict = pd.DataFrame(data_dict)\n    new_data = sm.add_constant(df_dict[[\"bed\", \"house_size\", \"zip_code_99350\"]])\n    \n    predictions = result.predict(new_data)\n    print(predictions)\n\n\nmain()"
  },
  {
    "objectID": "slides-04-01.html#removing-columns-from-data",
    "href": "slides-04-01.html#removing-columns-from-data",
    "title": "OLS Case Study",
    "section": "Removing columns from data",
    "text": "Removing columns from data\n\ncolumns_to_exclude = [\"price\"]\nselected_data = data.drop(columns=columns_to_exclude)"
  },
  {
    "objectID": "slides-04-01.html#selecting-some-columns-from-data",
    "href": "slides-04-01.html#selecting-some-columns-from-data",
    "title": "OLS Case Study",
    "section": "Selecting some columns from data",
    "text": "Selecting some columns from data\n\nselected_data  = filtered_data.loc[:, [\"price\", \"bed\", \"house_size\", \"zip_code\"]]"
  },
  {
    "objectID": "slides-04-02.html#data-wrangling",
    "href": "slides-04-02.html#data-wrangling",
    "title": "Logistic Regression",
    "section": "Data wrangling",
    "text": "Data wrangling\n\nMake every variable in Tagliamonte’s that expresion data a numeric variable\nCall your python script data_wrangling.py\nYour script should read data/that-expression.csv and write data/clean-that-expression.csv\nSubmit it to gradescope\n\n\nassert isinstance(data.iloc[:,0][0], np.int64)\nassert isinstance(data.iloc[:,1][0], np.int64)\nassert isinstance(data.iloc[:,2][0], np.int64)"
  },
  {
    "objectID": "project-01.html",
    "href": "project-01.html",
    "title": "Project 1",
    "section": "",
    "text": "In this project, you will be fitting a linear regression model to tuition in the US data.\nDownload the tuition data and set up your coding environment."
  },
  {
    "objectID": "project-01.html#data-wrangling",
    "href": "project-01.html#data-wrangling",
    "title": "Project 1",
    "section": "Data wrangling",
    "text": "Data wrangling\nCreate a data_wrangling.py Python script and unsure that:\n\nthe year variable is an integer – remove the - and everything after the dash\nthe state variable is transformed into dummy variables, with one variable for each state, one hot encoded (with zeros and 1, integer format)\nyour main() function reads the original file in a folder called data (use pd.read_csv(\"data/tuition.csv\")) and write out a clean-tuition.csv file to the data folder (use .to_csv(\"data/clean-tuition.csv\"))\n\nHere’s what your clean-tuition.csv file should look like (showing first rows and first columns only):"
  },
  {
    "objectID": "project-01.html#plot",
    "href": "project-01.html#plot",
    "title": "Project 1",
    "section": "Plot",
    "text": "Plot\nCreate a plots.ipynb python notebook file and with the clean-tuition.csv data create a scatterplot of tuition vs. academic year.\nYou should replicate this scatterplot:"
  },
  {
    "objectID": "project-01.html#submit-to-gradescope",
    "href": "project-01.html#submit-to-gradescope",
    "title": "Project 1",
    "section": "Submit to gradescope",
    "text": "Submit to gradescope\nYou are to submit three files to gradescope:\n\ndata_wrangling.py\nplots.ipynb\nmodeling.py\n\nYou can submit other .py files if you are using any – don’t include any subfolders in your submission though, gradescope will not be considering any folders when running the code (only the data folder where .csv files should be placed).\nThe data gradescope will be using is slightly different from the data you are being provided with, but your code should be generalizable (meaning, no hardcoding of model coefficients, for example).\nClick here to open gradescope assignment to submit your files"
  },
  {
    "objectID": "modules.html#assignment",
    "href": "modules.html#assignment",
    "title": "Modules",
    "section": "Assignment",
    "text": "Assignment\n\nProject 01 – due Friday, February 28, 11:59pm"
  },
  {
    "objectID": "project-01.html#data-modeling",
    "href": "project-01.html#data-modeling",
    "title": "Project 1",
    "section": "Data modeling",
    "text": "Data modeling\nCreate a modeling.py Python script and run Ordinary Least Squares on the clean-tution.csv data.\nEnsure that your main() function:\n\nprints out only the summary of the model, and the rsquared and rsquared adjusted values (in this order, your script should not print anything else) – you can use print(model.summary()), print(model.rsquared) and print(model.rsquared_adj) (replace model with the variable name you use in your script)\nwrites a coefficients.csv file with all of your model’s coefficients – you can use this block of code to based your code on (again, replace model with your variable name)\n\n\ncoeffs = model.params\ndf_coeffs = pd.DataFrame(coeffs)\ndf_coeffs.columns = [\"coefficient\"]\ndf_coeffs[\"variable\"] = df_coeffs.index\ndf_coeffs.to_csv(\"coefficients.csv\", index=False)\n\nHere’s what your coefficients.csv file should look like (showing first rows only):"
  },
  {
    "objectID": "slides-04-03.html#comparing-model-results",
    "href": "slides-04-03.html#comparing-model-results",
    "title": "OLS vs. Logit",
    "section": "Comparing model results",
    "text": "Comparing model results\nData: clean_titanic.csv\n\nCreate two Python scripts, one with OLS modeling, another with Logit modeling.\nWhat are the differences in the results?"
  },
  {
    "objectID": "slides-04-03.html#ols",
    "href": "slides-04-03.html#ols",
    "title": "OLS vs. Logit",
    "section": "OLS",
    "text": "OLS\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef logit_to_probability(logit):\n  return 1 / (1 + np.exp(-logit))\n\ndef main():\n    data = pd.read_csv(\"data/clean_titanic.csv\")\n\n    X = data[[\"fare\", \"sex\", \"age\", \n          \"siblings_spouses_aboard\",\n          \"parents_children_aboard\"]]\n    y = data[\"survived\"]\n\n    model = sm.OLS(y, sm.add_constant(X)).fit()\n    print(model.summary())\n\n    coeffs = pd.DataFrame(model.params)\n    print(coeffs)\n    coeffs_prob = coeffs[0].apply(logit_to_probability)\n    print(coeffs_prob)\n\nmain()"
  },
  {
    "objectID": "slides-04-03.html#logit",
    "href": "slides-04-03.html#logit",
    "title": "OLS vs. Logit",
    "section": "Logit",
    "text": "Logit\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef logit_to_probability(logit):\n  return 1 / (1 + np.exp(-logit))\n\ndef main():\n    data = pd.read_csv(\"data/clean_titanic.csv\")\n\n    X = data[[\"fare\", \"sex\", \"age\", \n          \"siblings_spouses_aboard\",\n          \"parents_children_aboard\"]]\n    y = data[\"survived\"]\n\n    model = sm.Logit(y, sm.add_constant(X)).fit()\n    print(model.summary())\n\n    coeffs = pd.DataFrame(model.params)\n    print(coeffs)\n    coeffs_prob = coeffs[0].apply(logit_to_probability)\n    print(coeffs_prob)\n\nmain()"
  },
  {
    "objectID": "slides-04-03.html#spam-vs.-ham",
    "href": "slides-04-03.html#spam-vs.-ham",
    "title": "OLS vs. Logit",
    "section": "Spam vs. Ham",
    "text": "Spam vs. Ham\nRun both OLS and Logit on this email data\n\nDownload the data\nClean the data\nWhat’s the target/response variable? What are the features/predictors?\nModel the data"
  },
  {
    "objectID": "slides-04-04.html#whats-machine-learning",
    "href": "slides-04-04.html#whats-machine-learning",
    "title": "Machine Learning (Prediction)",
    "section": "What’s machine learning?",
    "text": "What’s machine learning?\n\nmodeling to make predictions or find patterns in data\noften without explicit hypotheses\na flexible pattern-matcher looking for useful relationships in data\nmodels learn patterns and relationships within the data\nusually needs large amounts of data to train effective models"
  },
  {
    "objectID": "slides-04-04.html#scikit-learn",
    "href": "slides-04-04.html#scikit-learn",
    "title": "Machine Learning (Prediction)",
    "section": "Scikit-learn",
    "text": "Scikit-learn\n\noften shortened to sklearn\nopen-source machine learning library"
  },
  {
    "objectID": "slides-04-04.html#train-test",
    "href": "slides-04-04.html#train-test",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test",
    "text": "Train-Test\nSince the focus of machine learning is on model performance, we worry more about overfitting – we (validate and) test data that was not used in building the models (training)\nTesting (and validation) helps to assess how well the model generalizes to new, unseen data."
  },
  {
    "objectID": "slides-04-04.html#measuring-performance",
    "href": "slides-04-04.html#measuring-performance",
    "title": "Machine Learning (Prediction)",
    "section": "Measuring Performance",
    "text": "Measuring Performance\nRegression vs. Classification – different types of measures\nLet’s start with regression:\n\nMean Square Error (MSE):average of the squared differences between predicted and actual values\nSquare Root of MSE (RMSE): makes the error values more interpretable as they are in the same units as the target variable\nR-squared (R2 or \\(R^2\\)): the proportion of variance in the dependent variable explained by the model – ranges from 0 to 1, where 1 indicates a perfect fit."
  },
  {
    "objectID": "slides-04-04.html#train-test-with-sklearn",
    "href": "slides-04-04.html#train-test-with-sklearn",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\nWe first load the method for splitting the data:\n\nfrom sklearn.model_selection import train_test_split\n\nThen, after reading the data in, we calltrain_test_split()\n\ntrain_test_split(X, y, test_size=0.3)\n\nDocumentation for train_test_split"
  },
  {
    "objectID": "slides-04-04.html#whats-machine-learning-1",
    "href": "slides-04-04.html#whats-machine-learning-1",
    "title": "Machine Learning (Prediction)",
    "section": "What’s machine learning?",
    "text": "What’s machine learning?\n\nless concerned with whether the data represents a random sample from a population\nfocus on predictive accuracy and generalization performance\ninterested in how well the model performs on new data"
  },
  {
    "objectID": "slides-04-04.html#scikit-learn-1",
    "href": "slides-04-04.html#scikit-learn-1",
    "title": "Machine Learning (Prediction)",
    "section": "Scikit-learn",
    "text": "Scikit-learn\n\nThis package includes:\n\nSupervised Learning Algorithms: Linear Regression, Random Forests, Support Vector Machines (SVM), and Neural Networks\nUnsupervised Learning Clustering Algorithms: K-means, PCA\nModel Selection Tools: cross-validation, parameter tuning, and metrics evaluation\nData Preprocessing Tools: scalers, encoders, and feature selection"
  },
  {
    "objectID": "slides-04-04.html#train-test-1",
    "href": "slides-04-04.html#train-test-1",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test",
    "text": "Train-Test\n\nTraining Set:\n\nused to train the machine learning model\nmost of the data (70% to 80%, usually)\n\nTesting Set:\n\nused to evaluate the model’s performance\nsmaller portion of the data that the model has not seen during training."
  },
  {
    "objectID": "slides-04-04.html#train-test-with-sklearn-1",
    "href": "slides-04-04.html#train-test-with-sklearn-1",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\n\nrandom_state parameter: integer, seed value, to ensure reproducibility, sets the seed for the random number generator\n\nThe choice of random_state can impact the performance of your model, especially if the dataset is small or if the data points are not uniformly distributed.\nDifferent splits can lead to different training and testing sets, which in turn can affect the model’s performance metrics."
  },
  {
    "objectID": "slides-04-04.html#example",
    "href": "slides-04-04.html#example",
    "title": "Machine Learning (Prediction)",
    "section": "Example",
    "text": "Example\n\nimport random\nprint(random.randint(1, 10))\n\n7\n\n\n\nimport random\nrandom.seed(123)\nprint(random.randint(1, 10))\n\n1"
  },
  {
    "objectID": "slides-04-04.html#train-test-with-sklearn-2",
    "href": "slides-04-04.html#train-test-with-sklearn-2",
    "title": "Machine Learning (Prediction)",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\nLet’s use the clean email data for our first model (we will be running regression):\n\ndata = pd.read_csv(\"data/clean_email.csv\")\nX = data[[\"to_multiple\"]]\ny = data[\"spam\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
  },
  {
    "objectID": "slides-04-04.html#linear-regression",
    "href": "slides-04-04.html#linear-regression",
    "title": "Machine Learning (Prediction)",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe will be using LinearRegression from sklearn.linear_model\n\nfrom sklearn.linear_model import LinearRegression\n\nThen we create and fit the model to our data\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"
  },
  {
    "objectID": "slides-04-04.html#assessing-model",
    "href": "slides-04-04.html#assessing-model",
    "title": "Machine Learning (Prediction)",
    "section": "Assessing model",
    "text": "Assessing model\nWe make predictions based on the test features\n\ny_pred = model.predict(X_test)\n\nThen we evaluate the model – lets calculate MSE and R2.\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nThen we call the appropiate methods on our observed and predicted targets:\n\nprint(mean_squared_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))"
  },
  {
    "objectID": "slides-04-04.html#scaling-features",
    "href": "slides-04-04.html#scaling-features",
    "title": "Machine Learning (Prediction)",
    "section": "Scaling features",
    "text": "Scaling features\nLet’s start with StandardScaler\n\nfrom sklearn.preprocessing import StandardScaler\n\nWe then scale our features for both training and testing data sets.\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "slides-04-04.html#feature-scaling",
    "href": "slides-04-04.html#feature-scaling",
    "title": "Machine Learning (Prediction)",
    "section": "Feature scaling",
    "text": "Feature scaling\n\nPuts all features on a comparable level, so the model can properly assess their relative importance\nEnsures fair comparison between features – especially when features have very different scales (like age [0-100] vs. income [$0-$1,000,000]) (the model may incorrectly prioritize features with larger values)\nHelps avoid overflow or underflow issues in computations caused by very large or very small numbers can cause numerical"
  },
  {
    "objectID": "slides-04-04.html#feature-scaling-1",
    "href": "slides-04-04.html#feature-scaling-1",
    "title": "Machine Learning (Prediction)",
    "section": "Feature scaling",
    "text": "Feature scaling\nCommon scaling methods include:\n\nMin-Max Scaling: Scales features to a fixed range, usually [0,1]\nStandard Scaling: Transforms features to have zero mean and unit variance\nRobust Scaling: Uses statistics that are robust to outliers (like median and quartiles)"
  },
  {
    "objectID": "slides-05-01.html#train-test-with-sklearn",
    "href": "slides-05-01.html#train-test-with-sklearn",
    "title": "Introduction to Scikit-learn",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\nWe first load the method for splitting the data:\n\nfrom sklearn.model_selection import train_test_split\n\nThen, after reading the data in, we calltrain_test_split()\n\ntrain_test_split(X, y, test_size=0.3)\n\nDocumentation for train_test_split"
  },
  {
    "objectID": "slides-05-01.html#train-test-with-sklearn-1",
    "href": "slides-05-01.html#train-test-with-sklearn-1",
    "title": "Introduction to Scikit-learn",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\n\nrandom_state parameter: integer, seed value, to ensure reproducibility, sets the seed for the random number generator\n\nThe choice of random_state can impact the performance of your model, especially if the dataset is small or if the data points are not uniformly distributed.\nDifferent splits can lead to different training and testing sets, which in turn can affect the model’s performance metrics."
  },
  {
    "objectID": "slides-05-01.html#example",
    "href": "slides-05-01.html#example",
    "title": "Introduction to Scikit-learn",
    "section": "Example",
    "text": "Example\n\nimport random\nprint(random.randint(1, 10))\n\n1\n\n\n\nimport random\nrandom.seed(123)\nprint(random.randint(1, 10))\n\n1"
  },
  {
    "objectID": "slides-05-01.html#train-test-with-sklearn-2",
    "href": "slides-05-01.html#train-test-with-sklearn-2",
    "title": "Introduction to Scikit-learn",
    "section": "Train-Test with sklearn",
    "text": "Train-Test with sklearn\nLet’s use the clean email data for our first model (we will be running regression):\n\ndata = pd.read_csv(\"data/clean_email.csv\")\nX = data[[\"to_multiple\"]]\ny = data[\"spam\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
  },
  {
    "objectID": "slides-05-01.html#linear-regression",
    "href": "slides-05-01.html#linear-regression",
    "title": "Introduction to Scikit-learn",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe will be using LinearRegression from sklearn.linear_model\nFrom the implementation point of view, this is just plain Ordinary Least Squares (OLS)\n\nfrom sklearn.linear_model import LinearRegression\n\nThen we create and fit the model to our data\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)"
  },
  {
    "objectID": "slides-05-01.html#assessing-model",
    "href": "slides-05-01.html#assessing-model",
    "title": "Introduction to Scikit-learn",
    "section": "Assessing model",
    "text": "Assessing model\nWe make predictions based on the test features\n\ny_pred = model.predict(X_test)\n\nThen we evaluate the model – lets calculate MSE and R2.\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nThen we call the appropiate methods on our observed and predicted targets:\n\nprint(mean_squared_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))"
  },
  {
    "objectID": "slides-05-01.html#feature-scaling",
    "href": "slides-05-01.html#feature-scaling",
    "title": "Introduction to Scikit-learn",
    "section": "Feature scaling",
    "text": "Feature scaling\n\nPuts all features on a comparable level, so the model can properly assess their relative importance\nEnsures fair comparison between features – especially when features have very different scales (like age [0-100] vs. income [$0-$1,000,000]) (the model may incorrectly prioritize features with larger values)\nHelps avoid overflow or underflow issues in computations caused by very large or very small numbers can cause numerical"
  },
  {
    "objectID": "slides-05-01.html#feature-scaling-1",
    "href": "slides-05-01.html#feature-scaling-1",
    "title": "Introduction to Scikit-learn",
    "section": "Feature scaling",
    "text": "Feature scaling\nCommon scaling methods include:\n\nMin-Max Scaling: Scales features to a fixed range, usually [0,1]\nStandard Scaling: Transforms features to have zero mean and unit variance\nRobust Scaling: Uses statistics that are robust to outliers (like median and quartiles)"
  },
  {
    "objectID": "slides-05-01.html#scaling-features",
    "href": "slides-05-01.html#scaling-features",
    "title": "Introduction to Scikit-learn",
    "section": "Scaling features",
    "text": "Scaling features\nLet’s start with StandardScaler\n\nfrom sklearn.preprocessing import StandardScaler\n\nWe then scale our features for both training and testing data sets.\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "modules.html#slides-4",
    "href": "modules.html#slides-4",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nIntroduction to scikit-learn\nCase Studies"
  },
  {
    "objectID": "slides-05-01.html#assessing-model-linear-regression",
    "href": "slides-05-01.html#assessing-model-linear-regression",
    "title": "Introduction to Scikit-learn",
    "section": "Assessing model – linear regression",
    "text": "Assessing model – linear regression\nWe make predictions based on the test features\n\ny_pred = model.predict(X_test)\n\nThen we evaluate the model – lets calculate MSE and R2.\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nThen we call the appropiate methods on our observed and predicted targets:\n\nprint(mean_squared_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))"
  },
  {
    "objectID": "slides-05-01.html#logistic-regression",
    "href": "slides-05-01.html#logistic-regression",
    "title": "Introduction to Scikit-learn",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nWe will be using LogisticRegression from sklearn.linear_model\nFrom the implementation point of view, this is just plain Ordinary Least Squares (OLS)\n\nfrom sklearn.linear_model import LogisticRegression\n\nThen we create and fit the model to our data\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)"
  },
  {
    "objectID": "slides-05-01.html#assessing-classification",
    "href": "slides-05-01.html#assessing-classification",
    "title": "Introduction to Scikit-learn",
    "section": "Assessing Classification",
    "text": "Assessing Classification\n\nAccuracy – correct predictions / total predictions\nPrecision – (True Positives) / (True Positives + False Positives)\nRecall – (True Positives) / (True Positives + False Negatives)\nF1 Score – 2 * (Precision * Recall) / (Precision + Recall)"
  },
  {
    "objectID": "slides-05-01.html#assessing-classification-1",
    "href": "slides-05-01.html#assessing-classification-1",
    "title": "Introduction to Scikit-learn",
    "section": "Assessing Classification",
    "text": "Assessing Classification\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\nWe make predictions based on the test features\n\ny_pred = my_model.predict(X_test_scaled)\n\nThen we call the appropiate methods on our observed and predicted targets:\n\nprint(accuracy_score(y_test, y_pred))\n\n# precision -- how many messages were erronously identified as spam\n# (True Positives) / (True Positives + False Positives)\nprint(precision_score(y_test, y_pred))\n\n# recall -- how many spam messages were missed\n# (True Positives) / (True Positives + False Negatives)\nprint(recall_score(y_test, y_pred))\n\n# harmonic mean of precision and recall\nprint(f1_score(y_test, y_pred))"
  },
  {
    "objectID": "slides-05-01.html#confusion-matrix",
    "href": "slides-05-01.html#confusion-matrix",
    "title": "Introduction to Scikit-learn",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\ntable = pd.DataFrame({\"truth\": y_test, \"prediction\": y_pred})\ntable.to_csv(\"truth-predictions.csv\", index=False)\nprint(table.value_counts().reset_index())"
  },
  {
    "objectID": "slides-05-01.html#cross-validation",
    "href": "slides-05-01.html#cross-validation",
    "title": "Introduction to Scikit-learn",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nfrom sklearn.model_selection import cross_val_score\n\n\ncv_scores = cross_val_score(my_model, X_train_scaled, y_train, cv=10)"
  },
  {
    "objectID": "slides-05-01.html#costloss-function",
    "href": "slides-05-01.html#costloss-function",
    "title": "Introduction to Scikit-learn",
    "section": "Cost/Loss Function",
    "text": "Cost/Loss Function\n\na mathematical function that measures the difference between predicted outputs by the model and the actual target values\nthe goal is minimizing cost to improve accuracy during training\n\nLoss function: measure of error for a single training example\nCost function: average of the loss over an entire training data"
  },
  {
    "objectID": "slides-05-01.html#gradient-descent",
    "href": "slides-05-01.html#gradient-descent",
    "title": "Introduction to Scikit-learn",
    "section": "Gradient descent",
    "text": "Gradient descent\n\nused to minimize a function’s cost/loss\nfinds a local minimum of a differentiable function\nstart at a random point\nfind direction that descends\nstep size: can be larger at first, smaller as it descends"
  },
  {
    "objectID": "slides-05-02.html#data",
    "href": "slides-05-02.html#data",
    "title": "Case Studies",
    "section": "Data",
    "text": "Data\nWe will be working with Road Accident Survival Dataset"
  },
  {
    "objectID": "slides-05-02.html#data-cleaning",
    "href": "slides-05-02.html#data-cleaning",
    "title": "Case Studies",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nMake sure the data are in a format that can be modeled."
  },
  {
    "objectID": "slides-05-02.html#modeling",
    "href": "slides-05-02.html#modeling",
    "title": "Case Studies",
    "section": "Modeling",
    "text": "Modeling\n\nWhat is the target?\nWhat are the features?\nInferential or Machine learning?"
  },
  {
    "objectID": "slides-05-02.html#data-1",
    "href": "slides-05-02.html#data-1",
    "title": "Case Studies",
    "section": "Data",
    "text": "Data\nFatality in car crashes\n\nNo need to clean the data, it’s ready to go\nSubmit your predictions on the test.csv data as a my_predictions.csv file to gradescope – make sure you have a column named prediction in your file"
  },
  {
    "objectID": "slides-05-02.html#hyperparameters",
    "href": "slides-05-02.html#hyperparameters",
    "title": "Case Studies",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nalgorithms have a number of hyperparameters\nhyperparameters include solvers, regularization parameters, learning rate etc."
  },
  {
    "objectID": "slides-05-02.html#hyperparameter-tuning",
    "href": "slides-05-02.html#hyperparameter-tuning",
    "title": "Case Studies",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\n\nsystematically identify the optimal hyperparameter settings\n\nGridSearchCV documentation\n\nfrom sklearn.model_selection import GridSearchCV"
  },
  {
    "objectID": "slides-05-02.html#hyperparameter-tuning-with-gridsearchcv",
    "href": "slides-05-02.html#hyperparameter-tuning-with-gridsearchcv",
    "title": "Case Studies",
    "section": "Hyperparameter Tuning with GridSearchCV",
    "text": "Hyperparameter Tuning with GridSearchCV\nFirst we create a dictionary with paramenter options\n\nparam_grid = {\"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\",\n                             \"newton-cholesky\", \"sag\", \"saga\"],\n              \"penalty\": [\"l1\", \"l2\"]}\n\nAfter creating the model, we create are grid search object and fit the training data to it.\n\ngrid_search = GridSearchCV(model, param_grid, cv=10)\ngrid_search.fit(X_train_scaled, y_train)"
  },
  {
    "objectID": "slides-05-02.html#hyperparameter-tuning-with-gridsearchcv-1",
    "href": "slides-05-02.html#hyperparameter-tuning-with-gridsearchcv-1",
    "title": "Case Studies",
    "section": "Hyperparameter Tuning with GridSearchCV",
    "text": "Hyperparameter Tuning with GridSearchCV\nWe can access the best model, and evalauate it\n\nbest_model = grid_search.best_estimator_\nbest_params = grid_search.best_params_\n\naccuracy = best_model.score(X_test, y_test)\n\nprint(best_params)\nprint(accuracy)"
  },
  {
    "objectID": "project-02.html",
    "href": "project-02.html",
    "title": "Project 2",
    "section": "",
    "text": "In this project, you will modeling and predicting walkability index data.\n\nDownload the data files and set up your coding environment\nYou will find a dictionary file with the data, which provides variable descriptions\nThe target is the NatWalkInd variable – this is a continuous numeric variable\nYour model performance metrics should match the type of model you are fitting to the data\nGradescope is going to run two metrics: mean square error, and r-squared score – to get full points for this assignment, your predictions should have a mean square error lower than 1 and your r-squared score should be higher than 50%\nSubmit your predictions on the test.csv data as a my_predictions.csv file to gradescope – make sure you have a column named prediction in your file\n\nLink to Gradescope assignment"
  },
  {
    "objectID": "modules.html#assignment-1",
    "href": "modules.html#assignment-1",
    "title": "Modules",
    "section": "Assignment",
    "text": "Assignment\n\nProject 02 – due Friday, March 7, 11:59pm"
  },
  {
    "objectID": "slides-06-01.html#motivation-how-to-split-the-data",
    "href": "slides-06-01.html#motivation-how-to-split-the-data",
    "title": "Support Vector Machines",
    "section": "Motivation: how to split the data?",
    "text": "Motivation: how to split the data?\n\nbased on slides by Dr. Jiayin Wang"
  },
  {
    "objectID": "slides-06-01.html#data",
    "href": "slides-06-01.html#data",
    "title": "Support Vector Machines",
    "section": "Data",
    "text": "Data\nDownload the cupcake vs. muffin data"
  },
  {
    "objectID": "slides-06-01.html#motivation-how-to-split-the-data-1",
    "href": "slides-06-01.html#motivation-how-to-split-the-data-1",
    "title": "Support Vector Machines",
    "section": "Motivation: how to split the data?",
    "text": "Motivation: how to split the data?\nLinear regression: y = β₀ + β₁x"
  },
  {
    "objectID": "slides-06-01.html#motivation-how-to-split-the-data-2",
    "href": "slides-06-01.html#motivation-how-to-split-the-data-2",
    "title": "Support Vector Machines",
    "section": "Motivation: how to split the data?",
    "text": "Motivation: how to split the data?"
  },
  {
    "objectID": "slides-06-01.html#motivation-how-to-split-the-data-3",
    "href": "slides-06-01.html#motivation-how-to-split-the-data-3",
    "title": "Support Vector Machines",
    "section": "Motivation: how to split the data?",
    "text": "Motivation: how to split the data?\nMotivation of SVM: Use tools from optimization to find the best lines (or hyper planes or curves) that divide the two datasets."
  },
  {
    "objectID": "slides-06-01.html#motivation-how-to-split-the-data-4",
    "href": "slides-06-01.html#motivation-how-to-split-the-data-4",
    "title": "Support Vector Machines",
    "section": "Motivation: how to split the data?",
    "text": "Motivation: how to split the data?"
  },
  {
    "objectID": "slides-06-01.html#motivation-how-to-split-the-data-5",
    "href": "slides-06-01.html#motivation-how-to-split-the-data-5",
    "title": "Support Vector Machines",
    "section": "Motivation: how to split the data?",
    "text": "Motivation: how to split the data?"
  },
  {
    "objectID": "slides-06-01.html#motivation-how-to-split-the-data-6",
    "href": "slides-06-01.html#motivation-how-to-split-the-data-6",
    "title": "Support Vector Machines",
    "section": "Motivation: how to split the data?",
    "text": "Motivation: how to split the data?"
  },
  {
    "objectID": "slides-06-01.html#motivation-how-to-split-the-data-7",
    "href": "slides-06-01.html#motivation-how-to-split-the-data-7",
    "title": "Support Vector Machines",
    "section": "Motivation: how to split the data?",
    "text": "Motivation: how to split the data?"
  },
  {
    "objectID": "slides-06-01.html#the-data",
    "href": "slides-06-01.html#the-data",
    "title": "Support Vector Machines",
    "section": "The data",
    "text": "The data\nWe will be working with Mental Health Care data.\nDownload the clean data by biological sex.\nReplicate the plot below:"
  },
  {
    "objectID": "slides-06-01.html#motivation-how-to-split-the-data-8",
    "href": "slides-06-01.html#motivation-how-to-split-the-data-8",
    "title": "Support Vector Machines",
    "section": "Motivation: how to split the data?",
    "text": "Motivation: how to split the data?"
  },
  {
    "objectID": "slides-06-01.html#support-vector-machines-vms",
    "href": "slides-06-01.html#support-vector-machines-vms",
    "title": "Support Vector Machines",
    "section": "Support Vector Machines (VMS)",
    "text": "Support Vector Machines (VMS)"
  },
  {
    "objectID": "slides-06-01.html#support-vector-machines-vms-1",
    "href": "slides-06-01.html#support-vector-machines-vms-1",
    "title": "Support Vector Machines",
    "section": "Support Vector Machines (VMS)",
    "text": "Support Vector Machines (VMS)\n\nOptimization: maximize the margin\nConstrains: support vectors need to be away from the margin\nUseful when complex data that cannot be separated by a simple line\nMore computationally expensive"
  },
  {
    "objectID": "slides-06-01.html#fitting-a-svm-model",
    "href": "slides-06-01.html#fitting-a-svm-model",
    "title": "Support Vector Machines",
    "section": "Fitting a SVM model",
    "text": "Fitting a SVM model\nHere’s the import statement:\n\nfrom sklearn.svm import SVC\n\nCalling it:\n\nmodel = SVC()"
  },
  {
    "objectID": "slides-06-02.html#data",
    "href": "slides-06-02.html#data",
    "title": "Case Study",
    "section": "Data",
    "text": "Data\nDownload the cupcake vs. muffin data.\n\nQuestion: Are cupcakes and muffins the same thing?\n\nChallenge: Classify recipes as cupcakes or muffins. When given a new recipe, determine if it’s a cupcake or muffin."
  },
  {
    "objectID": "modules.html#slides-5",
    "href": "modules.html#slides-5",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nSupport Vector Machines\nCase Studies\nMulticlass Classification"
  },
  {
    "objectID": "slides-06-01.html",
    "href": "slides-06-01.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "based on slides by Dr. Jiayin Wang"
  },
  {
    "objectID": "slides-06-02.html#cupcake-vs.-muffin",
    "href": "slides-06-02.html#cupcake-vs.-muffin",
    "title": "Case Studies",
    "section": "Cupcake vs. Muffin",
    "text": "Cupcake vs. Muffin\nDownload the cupcake vs. muffin data.\n\nQuestion: Are cupcakes and muffins the same thing?\n\nChallenge: Classify recipes as cupcakes or muffins. When given a new recipe, determine if it’s a cupcake or muffin.\nSubmit your my_predictions.csv file to gradescope – make sure you have a column named prediction"
  },
  {
    "objectID": "slides-06-02.html#pumpkin-seeds",
    "href": "slides-06-02.html#pumpkin-seeds",
    "title": "Case Studies",
    "section": "Pumpkin Seeds",
    "text": "Pumpkin Seeds\nDownload and clean this pumpkin seeds data set"
  },
  {
    "objectID": "slides-06-02.html#decision-trees",
    "href": "slides-06-02.html#decision-trees",
    "title": "Case Studies",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nsupervised learning\na non-parametric algorithm – no specific functional form/pattern to fit, no distribution assumptions\nfor both classification and regression\ndivide and conquer strategy\nhierarchical, tree structure – root node, branches, internal nodes, leaf nodes"
  },
  {
    "objectID": "slides-06-02.html#decision-trees-1",
    "href": "slides-06-02.html#decision-trees-1",
    "title": "Case Studies",
    "section": "Decision Trees",
    "text": "Decision Trees\nDocumentation for Decision Trees\nExample of an import statement:\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nAnd then:\n\nmodel = DecisionTreeClassifier()"
  },
  {
    "objectID": "slides-06-02.html#random-forests",
    "href": "slides-06-02.html#random-forests",
    "title": "Case Studies",
    "section": "Random Forests",
    "text": "Random Forests\n\nCollection of decision trees\nEnsemble (voting) algorithm\nTrees work together to make more accurate and stable predictions\n\n\nfrom sklearn.ensemble import RandomForestClassifier"
  },
  {
    "objectID": "slides-06-02.html#confusion-matrix",
    "href": "slides-06-02.html#confusion-matrix",
    "title": "Case Studies",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\nprint(pd.DataFrame(confusion_matrix(y_test, y_pred)))"
  },
  {
    "objectID": "slides-06-02.html#roc-auc",
    "href": "slides-06-02.html#roc-auc",
    "title": "Case Studies",
    "section": "ROC AUC",
    "text": "ROC AUC\n\nROC AUC (Receiver Operating Characteristic – Area Under the Curve)\ncomprehensive way to measure a model’s performance across different classification thresholds\nAUC (Area Under the Curve) – the probability that a model will correctly rank a randomly chosen positive example higher than a randomly chosen negative example\nscore from 0 to 1 – a higher AUC == better model performance"
  },
  {
    "objectID": "slides-06-02.html#roc-auc-1",
    "href": "slides-06-02.html#roc-auc-1",
    "title": "Case Studies",
    "section": "ROC AUC",
    "text": "ROC AUC\n\ntrade-off between correctly identified positive cases (True Positives) and incorrectly identified negative cases (False Negative)"
  },
  {
    "objectID": "slides-06-02.html#roc-auc-2",
    "href": "slides-06-02.html#roc-auc-2",
    "title": "Case Studies",
    "section": "ROC AUC",
    "text": "ROC AUC\nCurve/plot:\n\ndefine multiple classification thresholds, and thencalculate:\n\nTrue Positive Rate (TPR)\nFalse Positive Rate (FPR)\n\nPlot points on the ROC curve\nCalculate the area under this curve"
  },
  {
    "objectID": "slides-06-02.html#roc-auc-3",
    "href": "slides-06-02.html#roc-auc-3",
    "title": "Case Studies",
    "section": "ROC AUC",
    "text": "ROC AUC\nImport statement:\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nUse:\n\nprint(roc_auc_score(y_test, y_pred))\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\nauc_df = pd.DataFrame({\"fpr\" : fpr, \"tpr\" : tpr})\nsns.lineplot(data = auc_df, x = \"fpr\", y = \"tpr\")\nplt.show()"
  },
  {
    "objectID": "slides-06-03.html#classification-of-targets-with-more-than-two-labels",
    "href": "slides-06-03.html#classification-of-targets-with-more-than-two-labels",
    "title": "Multiclass Classification",
    "section": "Classification of targets with more than two labels",
    "text": "Classification of targets with more than two labels\n\nSVC handles multiclass classification\nKNeighborsClassifier also handles multiple classes\nLogisticRegression does handle multiple classes as well"
  },
  {
    "objectID": "slides-06-03.html#case-study",
    "href": "slides-06-03.html#case-study",
    "title": "Multiclass Classification",
    "section": "Case study",
    "text": "Case study\nDownload this dermatology data and fit SVC to it"
  },
  {
    "objectID": "slides-06-03.html#classification-metrics",
    "href": "slides-06-03.html#classification-metrics",
    "title": "Multiclass Classification",
    "section": "Classification Metrics",
    "text": "Classification Metrics\nFor precision, recall, and F1 score we have three options:\n\nmacro – arithmetic/unweighted mean\nmicro – global, sums True Positives (TP), False Negatives (FN), and False Positives (FP)\nweighted – weighted mean, considering the number of occurrences for each class\n\n\nprint(f1_score(y_test, y_pred, average='macro'))"
  },
  {
    "objectID": "slides-06-03.html#classification-report",
    "href": "slides-06-03.html#classification-report",
    "title": "Multiclass Classification",
    "section": "Classification Report",
    "text": "Classification Report\n\nfrom sklearn.metrics import classification_report\nclassification_report(y_test, y_pred)"
  },
  {
    "objectID": "slides-06-03.html#classification-metrics-1",
    "href": "slides-06-03.html#classification-metrics-1",
    "title": "Multiclass Classification",
    "section": "Classification Metrics",
    "text": "Classification Metrics\n\nzero-one loss function – fraction of misclassifications – loss of 0 if the prediction is correct and 1 for incorrect\nreturns the fraction of misclassifications by default\nset normalize=False for count of missclassification cases\n\n\nfrom sklearn.metrics import zero_one_loss\nzero_one_loss(y_test, y_pred)"
  },
  {
    "objectID": "slides-06-02.html",
    "href": "slides-06-02.html",
    "title": "Case Studies",
    "section": "",
    "text": "Download the cupcake vs. muffin data.\n\nQuestion: Are cupcakes and muffins the same thing?\n\nChallenge: Classify recipes as cupcakes or muffins. When given a new recipe, determine if it’s a cupcake or muffin.\nSubmit your my_predictions.csv file to gradescope – make sure you have a column named prediction"
  },
  {
    "objectID": "slides-06-02.html#classification-metrics",
    "href": "slides-06-02.html#classification-metrics",
    "title": "Case Studies",
    "section": "Classification Metrics",
    "text": "Classification Metrics\n\nAccuracy – proportion of correctly classified instances among all instances\nPrecision – proportion of correctly classified positive instances among all instances predicted as positive (including false positives)\nRecall – proportion of correctly classified positive instances among all actual positive instances (including false negatives).\nF1-score – harmonic mean of precision and recall\nROC AUC – trade-off between correctly identified positive cases (true positives) and incorrectly identified negative cases (false negative)"
  },
  {
    "objectID": "slides-07-01.html#recap-supervised-vs.-unsupervised-learning",
    "href": "slides-07-01.html#recap-supervised-vs.-unsupervised-learning",
    "title": "Clustering",
    "section": "Recap: Supervised vs. Unsupervised Learning",
    "text": "Recap: Supervised vs. Unsupervised Learning\n\nSupervised Learning\n\nData: both the features, x, and a target, y, for each item in the dataset\nGoal: ‘learn’ how to predict the target from the features, y = f(x)\nExample: Regression and Classification"
  },
  {
    "objectID": "slides-07-01.html#recap-supervised-vs.-unsupervised-learning-1",
    "href": "slides-07-01.html#recap-supervised-vs.-unsupervised-learning-1",
    "title": "Clustering",
    "section": "Recap: Supervised vs. Unsupervised Learning",
    "text": "Recap: Supervised vs. Unsupervised Learning\n\nUnsupervised Learning\n\nData: Only the features, x, for each item in the dataset\nGoal: discover ‘interesting’ things about the dataset\nExample: Clustering, Dimensionality reduction, Principal Component Analysis (PCA)"
  },
  {
    "objectID": "slides-07-01.html#clustering",
    "href": "slides-07-01.html#clustering",
    "title": "Clustering",
    "section": "Clustering",
    "text": "Clustering\n\nClustering is the task of discovering unknown subgroups in data, or clusters\nThe goal is to partition the dataset into clusters where ‘similar’ items are in the same cluster and ‘dissimilar’ items are in different clusters\nExample:\n\nSocial Network Analysis: Clustering can be used to find communities\nHandwritten digits where the digits are unknown"
  },
  {
    "objectID": "slides-07-01.html#clustering-1",
    "href": "slides-07-01.html#clustering-1",
    "title": "Clustering",
    "section": "Clustering",
    "text": "Clustering\n\nPartition data into groups (clusters)\nPoints within a cluster should be “similar”\nPoints in different cluster should be “different”"
  },
  {
    "objectID": "slides-07-01.html#goal-of-clustering",
    "href": "slides-07-01.html#goal-of-clustering",
    "title": "Clustering",
    "section": "Goal of Clustering",
    "text": "Goal of Clustering\n\nData Exploration\n\nAre there coherent groups ?\nHow many groups are there ?\n\nData Partitioning\n\nDivide data by group before further processing"
  },
  {
    "objectID": "slides-07-01.html#formulation-of-k-means-clustering-method",
    "href": "slides-07-01.html#formulation-of-k-means-clustering-method",
    "title": "Clustering",
    "section": "Formulation of K-means Clustering Method",
    "text": "Formulation of K-means Clustering Method\n\nData: A collection of points \\(x_i\\), for i = 1,…,n\nFind k centers, assign each point x to a cluster as to minimize the total intra-cluster distance\nThe total intra-cluster distance is the total squared Euclidean distance from each point to the center of its cluster\nIt’s a measure of the variance or internal coherence of the cluster"
  },
  {
    "objectID": "slides-07-01.html#k-means-algorithm",
    "href": "slides-07-01.html#k-means-algorithm",
    "title": "Clustering",
    "section": "K-Means Algorithm",
    "text": "K-Means Algorithm\n\nPick number of clusters k\nPick k random points as “cluster center” (or centroid)\nWhile cluster centers change:\n\nAssign each data point to its closest cluster center\nRecompute cluster centers as the mean of the assigned points"
  },
  {
    "objectID": "slides-07-01.html#k-means-algorithm-1",
    "href": "slides-07-01.html#k-means-algorithm-1",
    "title": "Clustering",
    "section": "K-Means Algorithm",
    "text": "K-Means Algorithm"
  },
  {
    "objectID": "slides-07-01.html#k-means-algorithm-2",
    "href": "slides-07-01.html#k-means-algorithm-2",
    "title": "Clustering",
    "section": "K-Means Algorithm",
    "text": "K-Means Algorithm\n\nThe K-means clustering algorithm on Airbnb rentals in NYC.\nfrom Clustering With K-Means"
  },
  {
    "objectID": "slides-07-01.html#limitations-of-k-means",
    "href": "slides-07-01.html#limitations-of-k-means",
    "title": "Clustering",
    "section": "Limitations of K-Means",
    "text": "Limitations of K-Means\n\nRestriction to cluster shapes\nRestriction to distance of points to centers"
  },
  {
    "objectID": "slides-07-01.html#overview-of-clustering-methods",
    "href": "slides-07-01.html#overview-of-clustering-methods",
    "title": "Clustering",
    "section": "Overview of clustering methods",
    "text": "Overview of clustering methods\n\nfrom sklearn clustering documentation"
  },
  {
    "objectID": "slides-07-02.html#data",
    "href": "slides-07-02.html#data",
    "title": "Case Study",
    "section": "Data",
    "text": "Data\nDownload the data\nSet up your coding environment, clean the data"
  },
  {
    "objectID": "slides-07-02.html#data-cleaning-.apply",
    "href": "slides-07-02.html#data-cleaning-.apply",
    "title": "Case Study",
    "section": "Data Cleaning – .apply()",
    "text": "Data Cleaning – .apply()\nTo keep things organized, we can write functions to deal with individual values.\n\ndef live_alone(string):\n    if string in [\"Married\", \"Together\"]:\n        return 0\n    return 1\n\nThen you can use .apply() with the function we wrote:\n\ndata[\"Live_Alone\"] = data[\"Marital_Status\"].apply(live_alone)"
  },
  {
    "objectID": "slides-07-02.html#numpy-where",
    "href": "slides-07-02.html#numpy-where",
    "title": "Case Study",
    "section": "Numpy where()",
    "text": "Numpy where()\n\ndata[\"Parent\"] = np.where(data[\"Kidhome\"] + data[\"Teenhome\"]  &gt; 0, 1, 0)"
  },
  {
    "objectID": "slides-07-02.html#modeling",
    "href": "slides-07-02.html#modeling",
    "title": "Case Study",
    "section": "Modeling",
    "text": "Modeling\nLet’s start with KMeans\n\nfrom sklearn.cluster import KMeans\n\nWe can set the seed with np.random.seed()\n\nkmeans = KMeans(n_clusters = 6)\nX[\"Cluster\"] = kmeans.fit_predict(X)"
  },
  {
    "objectID": "slides-07-02.html#visualization",
    "href": "slides-07-02.html#visualization",
    "title": "Case Study",
    "section": "Visualization",
    "text": "Visualization\nImport both seaborn and matplotlib.pyplot\nVisualize clusters\n\nsns.scatterplot(x = \"Income\", y = \"Age\", data = X, hue = \"Cluster\")\nplt.show()"
  },
  {
    "objectID": "slides-07-01.html#dimensionality-reduction",
    "href": "slides-07-01.html#dimensionality-reduction",
    "title": "Clustering",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nIt is often beneficial to reduce number of dimensions before running K-means\nSimplification of the data – it makes it easier to find clusters\n\nCurse of Dimensionality\nAs the number of dimensions (features) in a dataset increases, the chances of two data points being close to each other become increasingly rare"
  },
  {
    "objectID": "slides-07-02.html#exploratory-visualization",
    "href": "slides-07-02.html#exploratory-visualization",
    "title": "Case Study",
    "section": "Exploratory Visualization",
    "text": "Exploratory Visualization\n\nsns.scatterplot(x = \"Income\", y = \"Recency\", hue = \"Parent\", data = data)"
  },
  {
    "objectID": "slides-07-02.html#preprocessing",
    "href": "slides-07-02.html#preprocessing",
    "title": "Case Study",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nScaling\nDimensionality Reduction"
  },
  {
    "objectID": "modules.html#slides-6",
    "href": "modules.html#slides-6",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nClustering\nCase Study\nClustering Images"
  },
  {
    "objectID": "final-project.html",
    "href": "final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "Summary of deliverables\nDeadline for all deliverables: May 2, Friday, 11:59pm\nDeliverables (everything should be in your GitHub repository):\n\n5 minute video presentation (in your GitHub repository)\nGitHub repo with all data and code\nA README.md file explaining what you did for your project (2 paragraphs)\n\nYou are to submit your link to your GitHub repository to Canvas.\n\n\nGitHub\n\nCreate a GitHub account:\n\nNavigate to https://github.com/.\nClick Sign up.\nFollow the prompts to create your personal account.\n\nCreate a new GitHub repository:\n\nClick “New repository”: Look for the “+ New repository” button in the upper-right corner of the page and click it.\nName your repository: Enter a short, memorable name for your repository.\nAdd a description (optional): You can add a brief description to explain what the repository is about.\nChoose repository visibility: Decide whether your repository should be public (visible to everyone) or private (only accessible to you and collaborators). If you decide on a private repository, make sure you share it with me (my GitHub handle is picoral)\nClick “Create repository”: Once you’ve filled in the details, click the “Create repository” button to finalize the process.\n\nAdd your files to your repository:\n\nClick to open your repository\nClick on Add File (top right of your repo file listing), then choose Upload Files\nDrag the files you want to add to your repo\nScroll down and click on Commit changes\n\n\nIf I am unable to access your repository to see your files, your grade will be zero.\n\n\nData (data_wrangling.py)\nYou will choose your your data source. Here are some options where you can find data:\n\nhttps://data.gov/\nhttps://data.europa.eu/en\nhttps://data.un.org\nhttps://data.worldbank.org\nhttps://www.who.int/data\n\nYou should have all of your python files in your GitHub repo, including data wrangling/cleaning scripts.\n\n\nVisualization (plots.ipynb)\nCreate a python notebook with at least two plots to visualize your raw data.\n\n\nModeling (modeling.py)\nCreate a python script with your modeling code.\nUse comments to explain what you did\n\n\nREADME (README.md)\nInclude in your README file information about your data and variables. What modeling approach you used and why. State your performance metrics, what you used, and what results you got."
  },
  {
    "objectID": "project-03.html",
    "href": "project-03.html",
    "title": "Project 3",
    "section": "",
    "text": "In this project, you will be creating plots for the customer data we worked with during lecture (March 17).\nYou can find the project with the data on canvas."
  },
  {
    "objectID": "project-03.html#plots",
    "href": "project-03.html#plots",
    "title": "Project 3",
    "section": "Plots",
    "text": "Plots\nCreate a plots.ipynb python notebook file and with the cleaned data create at least three plots that show some characteristics of the customers’ buying habits. Write a short paragraph for each plot explaining what we can see in the plot."
  },
  {
    "objectID": "project-03.html#submit-to-gradescope",
    "href": "project-03.html#submit-to-gradescope",
    "title": "Project 3",
    "section": "Submit to gradescope",
    "text": "Submit to gradescope\nYou are to submit three files to gradescope:\n\ndata_wrangling.py\nplots.ipynb\nmodeling.py\n\nYou can submit other .py files if you are using any – don’t include any subfolders in your submission though, gradescope will not be considering any folders when running the code (only the data folder where .csv files should be placed).\nClick here to open gradescope assignment to submit your files"
  },
  {
    "objectID": "modules.html#assignment-2",
    "href": "modules.html#assignment-2",
    "title": "Modules",
    "section": "Assignment",
    "text": "Assignment\n\nProject 03 – due Monday, March 24, 11:59pm"
  },
  {
    "objectID": "slides-07-03.html",
    "href": "slides-07-03.html",
    "title": "Clustering Images",
    "section": "",
    "text": "Download the digit images and set up your working environment."
  },
  {
    "objectID": "slides-07-03.html#data",
    "href": "slides-07-03.html#data",
    "title": "Clustering Images",
    "section": "Data",
    "text": "Data\nDownload the digit images and set up your working environment."
  },
  {
    "objectID": "slides-07-03.html#opening-images-in-python",
    "href": "slides-07-03.html#opening-images-in-python",
    "title": "Clustering Images",
    "section": "Opening images in Python",
    "text": "Opening images in Python\nWe will be using the Image module from PIL\n\nfrom PIL import Image\n\nIf you need to install PIL, you can run:\npython3 -m pip install --upgrade Pillow\nThen you can open an image:\n\nimage = Image.open(\"data/image.png\")\n\nUsing images in Python:\n\nimage.show()\nimage_array = np.array(image)\nimage.close()"
  },
  {
    "objectID": "slides-07-03.html#using-images-in-python",
    "href": "slides-07-03.html#using-images-in-python",
    "title": "Clustering Images",
    "section": "Using images in Python",
    "text": "Using images in Python\n\nimage = Image.open(path)\nimage_array = np.array(image)"
  },
  {
    "objectID": "slides-07-03.html#dealing-with-multiple-files",
    "href": "slides-07-03.html#dealing-with-multiple-files",
    "title": "Clustering Images",
    "section": "Dealing with multiple files",
    "text": "Dealing with multiple files\nWe will use the os module:\n\nimport os\n\nThen we can iterate over the list of files in a folder:\n\nfor filename in os.listdir(\"data\"):\n  if \".png\" in filename:\n    print(filename)"
  },
  {
    "objectID": "slides-07-03.html#how-to-transform-images-into-data",
    "href": "slides-07-03.html#how-to-transform-images-into-data",
    "title": "Clustering Images",
    "section": "How to transform images into data?",
    "text": "How to transform images into data?\nWe can get the values of individual pixels from each image.\nHow do we combine all images into one .csv file?"
  },
  {
    "objectID": "slides-07-03.html#how-to-transform-images-into-data-1",
    "href": "slides-07-03.html#how-to-transform-images-into-data-1",
    "title": "Clustering Images",
    "section": "How to transform images into data?",
    "text": "How to transform images into data?\n\nimage = Image.open(\"data/image.png\")\nimage_array = np.array(image)\n\nfor line in image_array:\n  for pixel in line:\n    pixels.append(pixel)"
  },
  {
    "objectID": "slides-07-03.html#function-to-deal-with-individual-image",
    "href": "slides-07-03.html#function-to-deal-with-individual-image",
    "title": "Clustering Images",
    "section": "Function to deal with individual image",
    "text": "Function to deal with individual image\n\ndef get_pixels(image_array):\n    pixels = []\n    for line in image_array:\n        for pixel in line:\n            pixels.append(pixel)\n    return pixels"
  },
  {
    "objectID": "slides-07-03.html#creating-the-data-frame",
    "href": "slides-07-03.html#creating-the-data-frame",
    "title": "Clustering Images",
    "section": "Creating the data frame",
    "text": "Creating the data frame\nFirst we create an empty data frame (there are 123 pixels in each image):\n\ndata = pd.DataFrame({\"index\" : list(range(123))})\n\nThen for each image, we create an individual data frame, calling the function we wrote previously:\n\nthese_data = pd.DataFrame({filename : get_pixels(image_array)})\n\nThen we concatenate the columns:\n\ndata = pd.concat([data, these_data], axis=1)"
  },
  {
    "objectID": "slides-07-03.html#creating-the-data-frame-1",
    "href": "slides-07-03.html#creating-the-data-frame-1",
    "title": "Clustering Images",
    "section": "Creating the data frame",
    "text": "Creating the data frame\nWhen we are done going over all images, and our data is complete, we transpose the data frame to make each column a row.\n\ndata_final = data.transpose()"
  },
  {
    "objectID": "slides-07-03.html#modeling",
    "href": "slides-07-03.html#modeling",
    "title": "Clustering Images",
    "section": "Modeling",
    "text": "Modeling\nNow we can cluster our data into 10 clusters.\n\nfrom sklearn.cluster import KMeans\n\n# read data in\ndata = pd.read_csv(\"all_pixels.csv\")\n\n# run KMeans with 10 clusters\nmodel = KMeans(n_clusters=10)\ndata[\"cluster\"]  = model.fit_predict(data)\n\n# save results\ndata.to_csv(\"digits.csv\", index=False)"
  },
  {
    "objectID": "slides-07-03.html#inspect-results",
    "href": "slides-07-03.html#inspect-results",
    "title": "Clustering Images",
    "section": "Inspect results",
    "text": "Inspect results\n\nimport pandas as pd\n\nfrom PIL import Image\n\ndef main():\n    data = pd.read_csv(\"digits.csv\")\n\n    print(data[\"cluster\"].value_counts())\n\n    for i in range(5):\n        Image.open(\"data/\" + data.iloc[i][\"filename\"] ).show()\n        print(data.iloc[i][\"cluster\"])\n\nmain()"
  },
  {
    "objectID": "slides-07-03.html#performance-metrics",
    "href": "slides-07-03.html#performance-metrics",
    "title": "Clustering Images",
    "section": "Performance metrics",
    "text": "Performance metrics\n\nHow do we measure performance without the ground truth?\n\nSilhouette Score\n\n-1 to 1 score (higher value indicates better clustering)\nMeasure of how similar data points are to their own cluster compared to other clusters"
  },
  {
    "objectID": "slides-07-03.html#silhouette-score",
    "href": "slides-07-03.html#silhouette-score",
    "title": "Clustering Images",
    "section": "Silhouette Score",
    "text": "Silhouette Score\n\nCombination of two factors:\n\nCohesion: How close a point is to other points in its cluster\nSeparation: How far a point is from points in other clusters"
  },
  {
    "objectID": "slides-07-03.html#silhouette-score-1",
    "href": "slides-07-03.html#silhouette-score-1",
    "title": "Clustering Images",
    "section": "Silhouette Score",
    "text": "Silhouette Score\nFor each data point:\n\nCalculate average distance to all other points in the same cluster (Cohesion)\nCalculate minimum average distance to points in any other cluster (Separation)\nThe silhouette value is (Separation - Cohesion) / max(Cohesion, Separation)\n\nThe overall Silhouette Score is the sum of the sillhouette values for all data points divided by the number of data points."
  },
  {
    "objectID": "slides-07-03.html#silhouette-score-limitations",
    "href": "slides-07-03.html#silhouette-score-limitations",
    "title": "Clustering Images",
    "section": "Silhouette Score – Limitations",
    "text": "Silhouette Score – Limitations\n\nComputationally intensive (especiall so for large datasets)\nFavors models with fewer clusters – Curse of Dimensionality"
  },
  {
    "objectID": "slides-07-03.html#silhouette-score-2",
    "href": "slides-07-03.html#silhouette-score-2",
    "title": "Clustering Images",
    "section": "Silhouette Score",
    "text": "Silhouette Score\nHere’s how to use silhouette_score from sklearn.metrics\n\nfrom sklearn.metrics import silhouette_score\n\n# calculate shilhouette score based on features and cluster labels\nprint(silhouette_score(X, data[\"cluster\"]))\n\nClose to zero: points are on or very close to the decision boundary between clusters"
  },
  {
    "objectID": "slides-07-02.html#dimensionality-reduction-pca",
    "href": "slides-07-02.html#dimensionality-reduction-pca",
    "title": "Case Study",
    "section": "Dimensionality Reduction – PCA",
    "text": "Dimensionality Reduction – PCA\nPrincipal Component Analysis (PCA):\n\na dimensionality reduction technique widely used in statistics, machine learning, and data science\nPCA transforms a dataset with potentially correlated variables into a set of linearly uncorrelated variables called principal components"
  },
  {
    "objectID": "slides-07-02.html#principal-component-analysis-pca",
    "href": "slides-07-02.html#principal-component-analysis-pca",
    "title": "Case Study",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nReduces the number of features in a dataset\nRetains most of the information\nIdentifies patterns and relationships between variables\nRemoves noise and redundancy from data\nUsed to preprocess data before using it for machine learning algorithms"
  },
  {
    "objectID": "slides-07-02.html#principal-component-analysis-pca-in-sklearn",
    "href": "slides-07-02.html#principal-component-analysis-pca-in-sklearn",
    "title": "Case Study",
    "section": "Principal Component Analysis (PCA) in sklearn",
    "text": "Principal Component Analysis (PCA) in sklearn\nHere’s the import statement:\n\nfrom sklearn.decomposition import PCA\n\nThen we can run it:\n\npca = PCA(n_components = 2)\n# fit the data\npca.fit(data)\n# get transformed data\nreduced_data = pd.DataFrame(pca.transform(data))"
  },
  {
    "objectID": "slides-07-02.html#visualize-results",
    "href": "slides-07-02.html#visualize-results",
    "title": "Case Study",
    "section": "Visualize results",
    "text": "Visualize results\n\nkmeans = KMeans(n_clusters = 6)\nreduced_data[\"Cluster\"] = kmeans.fit_predict(reduced_data)\n\nsns.scatterplot(x = 0, y = 1, data = reduced_data, hue = \"Cluster\")\nplt.show()"
  },
  {
    "objectID": "slides-08-01.html#neural-networks",
    "href": "slides-08-01.html#neural-networks",
    "title": "Introduction to Neural Networks",
    "section": "Neural Networks",
    "text": "Neural Networks\n\ncomputational models inspired by the human brain’s structure and function\ninterconnected nodes (neurons) organized in layers that process and transform input data to produce outputs\nStructure: an input layer, one or more hidden layers, and an output layer\nEach node receives inputs, applies weights, adds a bias, and passes the result through an activation function"
  },
  {
    "objectID": "slides-08-01.html#neural-networks-learning-process",
    "href": "slides-08-01.html#neural-networks-learning-process",
    "title": "Introduction to Neural Networks",
    "section": "Neural Networks – Learning Process",
    "text": "Neural Networks – Learning Process\n\nForward propagation: Input data flows through the network, generating predictions\nLoss calculation: The difference between predictions and actual values is measured\nBackpropagation: Errors are propagated backward to adjust weights\nGradient descent: Weights are updated to minimize the loss"
  },
  {
    "objectID": "slides-08-01.html#neural-networks-advantages",
    "href": "slides-08-01.html#neural-networks-advantages",
    "title": "Introduction to Neural Networks",
    "section": "Neural Networks – advantages",
    "text": "Neural Networks – advantages\n\nCan model complex non-linear relationships through multiple layers and activation functions\nCan model complex decision boundaries\nLearn feature representations automatically; work well with unstructured data like images and text\nPerform better as dataset size increases (many traditional algorithms plateau in performance)"
  },
  {
    "objectID": "slides-08-01.html#neural-networks-disadvantages",
    "href": "slides-08-01.html#neural-networks-disadvantages",
    "title": "Introduction to Neural Networks",
    "section": "Neural Networks – disadvantages",
    "text": "Neural Networks – disadvantages\n\nTraining neural networks typically requires significantly more computational resources\nNeural networks generally need large amounts of training data to perform well and avoid overfitting\nFinding optimal architectures and hyperparameter settings can be time-consuming and resource-intensive"
  },
  {
    "objectID": "modules.html#slides-7",
    "href": "modules.html#slides-7",
    "title": "Modules",
    "section": "Slides",
    "text": "Slides\n\nIntroduction to Neural Networks"
  },
  {
    "objectID": "slides-08-01.html#tensorflow",
    "href": "slides-08-01.html#tensorflow",
    "title": "Introduction to Neural Networks",
    "section": "Tensorflow",
    "text": "Tensorflow\nTo pip install tensorflow\n/path/to/python3 -m pip install tensorflow\nYou might need to downgrade numpy:\n/path/to/python3 -m pip install --upgrade numpy==1.26.4"
  },
  {
    "objectID": "slides-08-01.html#mnist-dataset",
    "href": "slides-08-01.html#mnist-dataset",
    "title": "Introduction to Neural Networks",
    "section": "MNIST dataset",
    "text": "MNIST dataset\n\n70,000 grayscale images of handwritten digits (0-9)\n60,000 data points for training\n10,000 for testing\ndataset from the National Institute of Standards and Technology (NIST)\nEach image is 28x28 pixels, representing a grayscale image with pixel values ranging from 0 (black) to 255 (white)\nEach image in the dataset is associated with a label representing the digit it depicts (0-9)"
  },
  {
    "objectID": "slides-08-01.html#mnist-dataset-1",
    "href": "slides-08-01.html#mnist-dataset-1",
    "title": "Introduction to Neural Networks",
    "section": "MNIST dataset",
    "text": "MNIST dataset\nWe will be using TensorFlow’s Keras API\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\nMac users: go to your Applications folder, find the python folder and double click on Install Certificates.command"
  },
  {
    "objectID": "slides-08-01.html#neural-networks-in-sklearn",
    "href": "slides-08-01.html#neural-networks-in-sklearn",
    "title": "Introduction to Neural Networks",
    "section": "Neural Networks in sklearn",
    "text": "Neural Networks in sklearn\nNeural Networks in sklearn"
  },
  {
    "objectID": "slides-08-01.html#neural-networks-some-terminology",
    "href": "slides-08-01.html#neural-networks-some-terminology",
    "title": "Introduction to Neural Networks",
    "section": "Neural Networks – Some terminology",
    "text": "Neural Networks – Some terminology\n\nReLU (Rectified Linear Unit) – activation function defined as f(x) = max(0, x), meaning it outputs x when x is positive and 0 when x is negative or zero. Helps prevent the vanishing gradient problem."
  },
  {
    "objectID": "slides-08-01.html#neural-networks-some-terminology-1",
    "href": "slides-08-01.html#neural-networks-some-terminology-1",
    "title": "Introduction to Neural Networks",
    "section": "Neural Networks – Some terminology",
    "text": "Neural Networks – Some terminology\n\nAdam (Adaptive Moment Estimation) – popular optimization algorithm for training neural networks, combines concepts from two other optimizers:\n\nMomentum: Accelerates learning in relevant directions by accumulating past gradients\nRMSprop: Adapts learning rates based on recent gradient magnitudes"
  },
  {
    "objectID": "slides-08-01.html#neural-networks-some-terminology-2",
    "href": "slides-08-01.html#neural-networks-some-terminology-2",
    "title": "Introduction to Neural Networks",
    "section": "Neural Networks – Some terminology",
    "text": "Neural Networks – Some terminology\n\nSoftmax function – exponentiates each output and then normalizes them so they can be interpreted as probabilities (values between 0 and 1 that sum to 1). This makes the model’s predictions more interpretable - each output value now represents the probability that the input belongs to the corresponding class."
  },
  {
    "objectID": "slides-08-01.html#neural-network",
    "href": "slides-08-01.html#neural-network",
    "title": "Introduction to Neural Networks",
    "section": "Neural Network",
    "text": "Neural Network\nWe will create a Sequential model, or a linear stack of layers using tf.keras.models.Sequential\n\nmodel = tf.keras.models.Sequential([\n  # transform the input from a 2D array of 28×28 pixels into a 1D array of 784 pixels\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  \n  # fully connected (Dense) layer with 128 neurons using 'relu' activation function\n  tf.keras.layers.Dense(128, activation='relu'),\n  \n  # a Dropout layer -- randomly sets 20% of the inputs to 0 during training\n  # helps prevent overfitting  (forces the network not to rely too heavily \n  # on any particular neuron)\n  tf.keras.layers.Dropout(0.2),\n  \n  # Dense layer with 10 neurons, corresponding to the output classes (digits 0-9)\n  tf.keras.layers.Dense(10)\n  ])"
  },
  {
    "objectID": "slides-08-01.html#neural-network-keras",
    "href": "slides-08-01.html#neural-network-keras",
    "title": "Introduction to Neural Networks",
    "section": "Neural Network – keras",
    "text": "Neural Network – keras\nWe will create a Sequential model, or a linear stack of layers using tf.keras.models.Sequential\n\nmodel = tf.keras.models.Sequential([\n  # transform the input from a 2D array of 28×28 pixels into a 1D array of 784 pixels\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  \n  # fully connected (Dense) layer with 128 neurons using 'relu' activation function\n  tf.keras.layers.Dense(128, activation='relu'),\n  \n  # a Dropout layer -- randomly sets 20% of the inputs to 0 during training\n  # helps prevent overfitting  (forces the network not to rely too heavily \n  # on any particular neuron)\n  tf.keras.layers.Dropout(0.2),\n  \n  # Dense layer with 10 neurons, corresponding to the output classes (digits 0-9)\n  tf.keras.layers.Dense(10)\n  ])"
  },
  {
    "objectID": "slides-08-01.html#neural-network-keras-1",
    "href": "slides-08-01.html#neural-network-keras-1",
    "title": "Introduction to Neural Networks",
    "section": "Neural Network – keras",
    "text": "Neural Network – keras\nWe now can compile our model. We will define an optimizer (adam is a good choice), and a loss function (Sparse Categorical Crossentropy for integer labels)\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nmodel.compile(optimizer=\"adam\",\nloss=loss_fn,\nmetrics=[\"accuracy\"])"
  },
  {
    "objectID": "slides-08-01.html#neural-network-keras-2",
    "href": "slides-08-01.html#neural-network-keras-2",
    "title": "Introduction to Neural Networks",
    "section": "Neural Network – keras",
    "text": "Neural Network – keras\nWe will fit the model with the training data with 10 epochs.\n\nmodel.fit(x_train, y_train, epochs=10)\n\nWe evaluate the model with the test data (verbose: 0 – nothing, 1 – progress bar, 2 – one line per epoch)\n\nmodel.evaluate(x_test,  y_test, verbose=2)"
  },
  {
    "objectID": "slides-08-01.html#neural-network-keras-3",
    "href": "slides-08-01.html#neural-network-keras-3",
    "title": "Introduction to Neural Networks",
    "section": "Neural Network – keras",
    "text": "Neural Network – keras\nWe will create a new model that builds upon the previously defined neural network by adding an additional Softmax activation function as a new final layer to the model. This added a Softmax layer converts the outputs to probabilities that sum to 1 across all classes.\n\nprobability_model = tf.keras.Sequential([\n  model,\n  tf.keras.layers.Softmax()\n  ])\n\n# get labels for first 5 test data points\nprint(probability_model(x_test[:5]))"
  },
  {
    "objectID": "slides-08-01.html#neural-networks-some-terminology-3",
    "href": "slides-08-01.html#neural-networks-some-terminology-3",
    "title": "Introduction to Neural Networks",
    "section": "Neural Networks – Some terminology",
    "text": "Neural Networks – Some terminology\n\nEpochs – one complete pass through the entire training dataset; multiple epochs are typically needed for a network to learn effectively. Too few epochs == underfitting; too many == overfitting."
  },
  {
    "objectID": "slides-08-01.html#another-case-study",
    "href": "slides-08-01.html#another-case-study",
    "title": "Introduction to Neural Networks",
    "section": "Another case study",
    "text": "Another case study\nFashion-MNIST\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
  }
]